{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Quick Metric","text":"<p>Welcome to Quick Metric - a powerful framework for quickly creating metrics using easy-to-edit YAML configs and reusable methods to filter, calculate, and transform data.</p> <p>Purpose</p> <p>Quick Metric empowers data scientists and analysts to:</p> <ul> <li>Define custom metric methods using simple decorators</li> <li>Configure complex data filtering via YAML or dictionary configurations</li> <li>Apply multiple metrics to pandas DataFrames consistently</li> <li>Integrate metrics generation into data processing pipelines</li> <li>Keep metric definitions separate from data processing logic</li> </ul>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>@metric_method Decorator - Register custom metric functions with a simple decorator</li> <li>Core Functions - Main entry points: <code>generate_metrics()</code> and <code>interpret_metric_instructions()</code></li> <li>Multiple Output Formats - Results as nested dict, pandas DataFrame, or list of records</li> <li>Data Filtering - Complex filtering logic with YAML configuration support</li> <li>Method Application - Execute methods on filtered data with error handling</li> <li>Pipeline Integration - Seamless integration with oops-its-a-pipeline workflows</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Install Quick Metric and its dependencies:</p> <pre><code># Clone and install\ngit clone &lt;repository-url&gt;\ncd quick_metric\nuv venv &amp;&amp; source .venv/bin/activate\nuv pip install -e .\n</code></pre> <p>Basic usage example:</p> <pre><code>from quick_metric import metric_method, generate_metrics\nimport pandas as pd\n\n# Define custom metric methods\n@metric_method\ndef count_records(data):\n    \"\"\"Count the number of records.\"\"\"\n    return len(data)\n\n@metric_method  \ndef mean_value(data, column='value'):\n    \"\"\"Calculate mean of a column.\"\"\"\n    return data[column].mean() if column in data.columns else 0.0\n\n# Create data and configuration\ndata = pd.DataFrame({\n    'category': ['A', 'B', 'A', 'C'],\n    'value': [10, 20, 15, 30],\n    'status': ['active', 'inactive', 'active', 'active']\n})\n\nconfig = {\n    'active_category_a': {\n        'method': ['count_records', 'mean_value'],\n        'filter': {\n            'and': {\n                'category': 'A',\n                'status': 'active'\n            }\n        }\n    }\n}\n\n# Generate metrics\nresults = generate_metrics(data, config)\nprint(results['active_category_a']['count_records'])  # 2\nprint(results['active_category_a']['mean_value'])     # 12.5\n</code></pre>"},{"location":"#output-formats","title":"Output Formats","text":"<p>Quick Metric supports multiple output formats to suit different use cases:</p> <pre><code># Default nested dictionary format (backward compatible)\nnested_results = generate_metrics(data, config)\n# Returns: {'metric_name': {'method_name': result}}\n\n# DataFrame format (perfect for analysis and visualization)\ndf_results = generate_metrics(data, config, output_format=\"dataframe\")\n# Returns: pandas DataFrame with columns [metric, method, value, value_type]\n\n# Records format (ideal for APIs and databases)\nrecords_results = generate_metrics(data, config, output_format=\"records\")\n# Returns: [{'metric': 'metric_name', 'method': 'method_name', 'value': result}]\n</code></pre>"},{"location":"#pipeline-integration","title":"Pipeline Integration","text":"<p>Quick Metric integrates seamlessly with oops-its-a-pipeline:</p> <pre><code>from oops_its_a_pipeline import Pipeline, PipelineConfig\nfrom quick_metric.pipeline import create_metrics_stage\n\nclass Config(PipelineConfig):\n    model_config = {'arbitrary_types_allowed': True}\n    data = your_dataframe\n    config = your_metrics_config\n\npipeline = Pipeline(Config()).add_stage(create_metrics_stage())\nresults = pipeline.run(\"analysis\")\n</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":"<p>New to Quick Metric? Start with our Getting Started guide to learn the fundamentals and begin using the framework in your analytical workflows.</p>"},{"location":"#api-reference","title":"API Reference","text":"<p>Explore the complete API Reference for detailed documentation of all modules, classes, and functions.</p>"},{"location":"configuration/","title":"Configuration Guide","text":"<p>This guide covers advanced configuration techniques, filtering syntax, and method parameters for Quick Metric.</p>"},{"location":"configuration/#configuration-formats","title":"Configuration Formats","text":"<p>Quick Metric uses YAML configuration files by default, with dictionary configuration available for programmatic use.</p> YAML Configuration (Default)Dictionary Configuration <p>YAML is the primary configuration format for Quick Metric, providing a clean and readable way to define metrics:</p> <pre><code>metric_instructions:\n  cancer_metrics:\n    method: ['count_records', 'mean_value']\n    filter:\n      and:\n        disease_type: Cancer\n        status: Active\n        not:\n          remove: Remove\n\n  rare_disease_metrics:\n    method: ['count_records']\n    filter:\n      and:\n        disease_type: 'Rare Disease'\n        status: Active\n</code></pre> <p>YAML configurations are loaded using:</p> <pre><code>from quick_metric import generate_metrics\n\n# Load from YAML file\nresults = generate_metrics(data, \"config/metrics.yaml\")\n</code></pre> <p>For programmatic use and dynamic configuration, dictionaries provide maximum flexibility:</p> <pre><code>config = {\n    'metric_name': {\n        'method': ['method1', 'method2'],\n        'filter': {\n            'column': 'value',\n            'numeric_column': {'&gt;=': 100},\n            'and': {\n                'status': 'active',\n                'not': {'remove': 'Remove'}\n            }\n        }\n    }\n}\n\n# Use directly with generate_metrics\nresults = generate_metrics(data, config)\n</code></pre>"},{"location":"configuration/#advanced-filtering","title":"Advanced Filtering","text":""},{"location":"configuration/#logical-operators","title":"Logical Operators","text":"<pre><code># AND conditions\n'filter': {\n    'and': {\n        'status': 'active',\n        'category': 'A',\n        'value': {'&gt;=': 100}\n    }\n}\n\n# OR conditions  \n'filter': {\n    'or': {\n        'priority': 'high',\n        'value': {'&gt;=': 1000}\n    }\n}\n\n# NOT conditions\n'filter': {\n    'not': {\n        'status': 'deleted'\n    }\n}\n\n# Complex nested logic\n'filter': {\n    'and': {\n        'status': 'active',\n        'or': {\n            'category': 'premium',\n            'value': {'&gt;=': 500}\n        },\n        'not': {\n            'flag': 'exclude'\n        }\n    }\n}\n</code></pre>"},{"location":"configuration/#comparison-operators","title":"Comparison Operators","text":"<pre><code>'filter': {\n    'numeric_value': {'&gt;=': 100},      # Greater than or equal\n    'other_value': {'&lt;=': 50},         # Less than or equal\n    'exact_value': {'==': 42},         # Exact match\n    'not_value': {'!=': 0},            # Not equal\n    'range_value': {'&gt;=': 10, '&lt;=': 100}  # Range (AND condition)\n}\n</code></pre>"},{"location":"configuration/#working-with-lists-and-multiple-values","title":"Working with Lists and Multiple Values","text":"<pre><code># Multiple acceptable values (OR condition)\n'filter': {\n    'category': ['A', 'B', 'C']  # category in ['A', 'B', 'C']\n}\n\n# Exclude multiple values\n'filter': {\n    'not': {\n        'status': ['deleted', 'archived', 'hidden']\n    }\n}\n</code></pre>"},{"location":"configuration/#method-parameters","title":"Method Parameters","text":""},{"location":"configuration/#parameterized-methods","title":"Parameterized Methods","text":"<p>Quick Metric supports method parameters through the configuration. You can specify parameters using a dictionary format:</p> <pre><code>@metric_method\ndef percentile_value(data, column='value', percentile=50):\n    \"\"\"Calculate percentile of a column.\"\"\"\n    return data[column].quantile(percentile / 100)\n\n@metric_method  \ndef top_n_sum(data, column='value', n=10):\n    \"\"\"Sum of top N values.\"\"\"\n    return data.nlargest(n, column)[column].sum()\n\n# Configuration with parameters\nconfig = {\n    'quartile_analysis': {\n        'method': [\n            {'percentile_value': {'percentile': 25}},\n            {'percentile_value': {'percentile': 75}},\n            {'top_n_sum': {'n': 5}}\n        ],\n        'filter': {'status': 'active'}\n    }\n}\n</code></pre>"},{"location":"configuration/#mixed-configurations","title":"Mixed Configurations","text":"<p>You can mix parameterized and non-parameterized methods in the same configuration:</p> <pre><code>config = {\n    'analysis': {\n        'method': [\n            'percentile_value',  # Uses default percentile=50\n            {'percentile_value': {'percentile': 90}},  # Custom percentile\n            {'top_n_sum': {'n': 3}}  # Custom n value\n        ],\n        'filter': {'status': 'active'}\n    }\n}\n</code></pre>"},{"location":"configuration/#result-keys","title":"Result Keys","text":"<p>When using parameters, the result keys include the parameter values for uniqueness:</p> <ul> <li><code>percentile_value</code> (default parameters) \u2192 <code>percentile_value</code></li> <li><code>{'percentile_value': {'percentile': 25}}</code> \u2192 <code>percentile_value_percentile25</code></li> <li><code>{'top_n_sum': {'n': 5}}</code> \u2192 <code>top_n_sum_n5</code></li> </ul>"},{"location":"configuration/#default-parameters","title":"Default Parameters","text":"<p>Methods with default parameters will use those defaults when called without explicit parameters:</p> <pre><code>@metric_method\ndef value_above_threshold(data, column='value', threshold=100):\n    \"\"\"Count records above threshold.\"\"\"\n    return len(data[data[column] &gt; threshold])\n\n# Configuration using default and custom parameters\nconfig = {\n    'threshold_analysis': {\n        'method': [\n            'value_above_threshold',  # Uses threshold=100 (default)\n            {'value_above_threshold': {'threshold': 50}},  # Custom threshold\n            {'value_above_threshold': {'threshold': 200}}  # Another custom threshold\n        ],\n        'filter': {}\n    }\n}\n</code></pre>"},{"location":"configuration/#parameter-validation","title":"Parameter Validation","text":"<p>The system validates that:</p> <ul> <li>Method dictionaries contain exactly one method</li> <li>Parameters are provided as dictionaries</li> <li>Method names exist in the registry</li> <li>Parameters match the method signature</li> </ul> <pre><code># \u2705 Valid configurations\n{'method_name': {'param': value}}\n'method_name'\n\n# \u274c Invalid configurations  \n{'method1': {}, 'method2': {}}  # Multiple methods in one dict\n{'method_name': 'not_a_dict'}   # Parameters not a dict\n</code></pre>"},{"location":"configuration/#tips-and-best-practices","title":"Tips and Best Practices","text":""},{"location":"configuration/#performance-optimization","title":"Performance Optimization","text":"<ol> <li>Order filters by selectivity - Put most selective filters first</li> <li>Use appropriate data types - Ensure numeric comparisons use numeric types</li> <li>Minimize method complexity - Break complex calculations into smaller methods</li> </ol>"},{"location":"configuration/#configuration-management","title":"Configuration Management","text":"<ol> <li>Use descriptive metric names - Makes results easier to interpret</li> <li>Group related metrics - Organize configurations by business domain</li> <li>Document complex filters - Add comments in YAML for clarity</li> <li>Version control configurations - Track changes to metric definitions</li> </ol>"},{"location":"contributing/","title":"Contributing to Quick Metric","text":"<p>We welcome contributions to Quick Metric! This guide outlines our development practices and how to contribute effectively.</p>"},{"location":"contributing/#development-philosophy","title":"Development Philosophy","text":"<p>Quick Metric follows modern Python development practices to ensure code quality, maintainability, and reliability.</p>"},{"location":"contributing/#code-standards","title":"Code Standards","text":""},{"location":"contributing/#formatting-and-linting","title":"Formatting and Linting","text":"<ul> <li>Tool: <code>ruff</code> for both formatting and linting</li> <li>Configuration: Defined in <code>pyproject.toml</code></li> <li>Line Length: 100 characters</li> <li>Import Sorting: Handled by ruff's isort integration</li> </ul> <pre><code>uv run ruff format quick_metric/ tests/ # (1)!\nuv run ruff check quick_metric/ tests/ # (2)!\n</code></pre> <ol> <li>Format code automatically according to project standards</li> <li>Check for linting issues and code quality problems</li> </ol>"},{"location":"contributing/#documentation","title":"Documentation","text":"<ul> <li>Docstring Format: NumPy-style docstrings</li> <li>Documentation Tool: <code>mkdocs</code> with Material theme</li> <li>API Documentation: Auto-generated with <code>mkdocstrings</code></li> </ul> <p>Example docstring format:</p> <pre><code>def calculate_metric(data: pd.DataFrame, method: str) -&gt; float:\n    \"\"\"Calculate a metric using the specified method.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The input DataFrame containing the data to process.\n    method : str\n        The name of the calculation method to apply.\n\n    Returns\n    -------\n    float\n        The calculated metric value.\n\n    Raises\n    ------\n    ValueError\n        If the specified method is not recognized.\n    \"\"\"\n</code></pre>"},{"location":"contributing/#testing-standards","title":"Testing Standards","text":""},{"location":"contributing/#testing-framework","title":"Testing Framework","text":"<ul> <li>Tool: <code>pytest</code> with coverage reporting</li> <li>Structure: Class-based test organization with separate <code>unit/</code> and <code>e2e/</code> test directories</li> <li>Coverage: Aim for &gt;90% test coverage</li> <li>Fixtures: Defined in <code>conftest.py</code> files</li> </ul>"},{"location":"contributing/#test-organization","title":"Test Organization","text":"<pre><code>tests/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 conftest.py          # Shared fixtures\n\u251c\u2500\u2500 unit/                # Unit tests (fast, isolated)\n\u2502   \u251c\u2500\u2500 test_core.py\n\u2502   \u251c\u2500\u2500 test_filters.py\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 e2e/                 # End-to-end tests (integration)\n    \u251c\u2500\u2500 test_pipeline.py\n    \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"contributing/#running-tests","title":"Running Tests","text":"<pre><code>uv run python -m pytest # (1)!\nuv run python -m pytest --cov=quick_metric # (2)!\nuv run python -m pytest tests/unit/ # (3)!\nuv run python -m pytest tests/e2e/ # (4)!\n</code></pre> <ol> <li>Run all tests in the project</li> <li>Run tests with coverage reporting</li> <li>Run only unit tests (fast, isolated tests)</li> <li>Run only end-to-end tests (integration tests)</li> </ol>"},{"location":"contributing/#test-organization-pattern","title":"Test Organization Pattern","text":"<p>Tests are organized using classes, with each function being tested having its own class:</p> <pre><code>class TestGenerateMetrics:\n    \"\"\"Test cases for the generate_metrics function.\"\"\"\n\n    def test_generate_metrics_with_valid_config(self):\n        \"\"\"Test generate_metrics with a valid configuration.\"\"\"\n\n    def test_generate_metrics_raises_error_with_invalid_method(self):\n        \"\"\"Test generate_metrics raises error for invalid method.\"\"\"\n\nclass TestFilterData:\n    \"\"\"Test cases for the filter_data function.\"\"\"\n\n    def test_filter_data_with_simple_condition(self):\n        \"\"\"Test filter_data with a simple condition.\"\"\"\n</code></pre>"},{"location":"contributing/#test-naming-convention","title":"Test Naming Convention","text":"<ul> <li>Test files: <code>test_&lt;module_name&gt;.py</code></li> <li>Test classes: <code>Test&lt;FunctionName&gt;</code> (one class per function being tested)</li> <li>Test methods: <code>test_&lt;function_being_tested&gt;_&lt;scenario&gt;</code></li> </ul>"},{"location":"contributing/#dependency-management","title":"Dependency Management","text":""},{"location":"contributing/#tool","title":"Tool","text":"<ul> <li>Primary: <code>uv</code> for fast dependency resolution and virtual environment management</li> <li>Fallback: <code>pip</code> is supported but <code>uv</code> is preferred</li> </ul>"},{"location":"contributing/#adding-dependencies","title":"Adding Dependencies","text":"<ol> <li>Add to <code>pyproject.toml</code> in the appropriate section:</li> <li><code>dependencies</code> for runtime dependencies</li> <li><code>dev</code> for development dependencies</li> <li> <p><code>docs</code> for documentation dependencies</p> </li> <li> <p>Update the lock file:</p> </li> </ol> <pre><code># Update dependency lock file after changes\nuv lock\n</code></pre>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Fork and Clone: Fork the repository and clone your fork</li> <li>Branch: Create a feature branch from <code>main</code></li> <li>Develop: Make your changes following the code standards</li> <li>Test: Ensure all tests pass and add new tests for your changes</li> <li>Format: Run <code>ruff</code> formatting and linting</li> <li>Documentation: Update documentation if needed</li> <li>Commit: Use clear, descriptive commit messages</li> <li>Pull Request: Create a PR with a clear description</li> </ol>"},{"location":"contributing/#commit-message-format","title":"Commit Message Format","text":"<pre><code>type(scope): brief description\n\nLonger explanation if needed\n\n- Bullet points for multiple changes\n- Reference issues: Fixes #123\n</code></pre> <p>Types: <code>feat</code>, <code>fix</code>, <code>docs</code>, <code>style</code>, <code>refactor</code>, <code>test</code>, <code>chore</code></p>"},{"location":"contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"contributing/#setup","title":"Setup","text":"<pre><code>git clone https://github.com/your-username/quick_metric.git # (1)!\ncd quick_metric # (2)!\nuv venv # (3)!\nsource .venv/bin/activate # (4)!\nuv pip install -e \".[dev,docs]\" # (5)!\nuv pip install pre-commit # (6)!\nuv run pre-commit install # (7)!\n</code></pre> <ol> <li>Clone your fork of the repository</li> <li>Navigate to the project directory</li> <li>Create a virtual environment using uv</li> <li>Activate the virtual environment (On Windows: <code>.venv\\Scripts\\activate</code>)</li> <li>Install the package in development mode with all dependencies</li> <li>Install pre-commit for code quality hooks</li> <li>Set up pre-commit hooks to run automatically on commits</li> </ol>"},{"location":"contributing/#pre-commit-checks","title":"Pre-commit Checks","text":"<p>Before committing, run these quality checks:</p> <pre><code>uv run ruff format quick_metric/ tests/ # (1)!\nuv run ruff check quick_metric/ tests/ # (2)!\nuv run python -m pytest --cov=quick_metric # (3)!\nmake lint test # (4)!\n</code></pre> <ol> <li>Format code automatically</li> <li>Check for linting and code quality issues</li> <li>Run tests with coverage reporting</li> <li>Alternative: Use Makefile commands for all checks</li> </ol>"},{"location":"contributing/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>We use pre-commit hooks to automatically check code quality:</p> <ul> <li>ruff: Formatting and linting</li> <li>ruff-format: Code formatting</li> </ul> <p>The hooks run automatically on <code>git commit</code> and will prevent commits that don't meet our standards.</p>"},{"location":"contributing/#getting-help","title":"Getting Help","text":"<ul> <li>Issues: Use GitHub Issues for bug reports and feature requests</li> <li>Discussions: Use GitHub Discussions for questions and ideas</li> <li>Documentation: Check the docs at [project documentation site]</li> </ul> <p>Thank you for contributing to Quick Metric! \ud83d\ude80</p>"},{"location":"getting_started/","title":"Getting Started","text":"<p>This guide will help you install Quick Metric and set up your development environment.</p>"},{"location":"getting_started/#prerequisites","title":"Prerequisites","text":"Requirement Version Description Python 3.9+ Required for modern type hints and dependency compatibility Git Latest Version control system for cloning the repository uv (Optional) Latest Fast Python package manager (recommended for dependency management) <p>Pipeline Integration</p> <p>Quick Metric includes optional support for pipeline integration with <code>oops-its-a-pipeline</code>. This dependency is automatically installed and enables advanced workflow capabilities. All pipeline functionality is optional for standalone metric processing.</p>"},{"location":"getting_started/#installation","title":"Installation","text":"<p>Choose your preferred installation method:</p> Git Installation (Recommended)PyPI <p>Install directly from the GitHub repository:</p> uv (Recommended)pip <pre><code>uv add git+https://github.com/nhsengland/quick_metric.git\n</code></pre> <pre><code>pip install git+https://github.com/nhsengland/quick_metric.git\n</code></pre> <p>Not Yet Available</p> <p>PyPI installation is not yet available but will be supported in future releases.</p> pipuv <pre><code>pip install quick-metric\n</code></pre> <pre><code>uv add quick-metric\n</code></pre> <p>Development Installation</p> <p>For development work, see the Development Setup section below.</p> <p>Quick Start Example</p> <p>Here's a minimal example to verify your installation:</p> <pre><code>from quick_metric import metric_method, generate_metrics\nimport pandas as pd\n\n@metric_method\ndef count_records(data):\n    \"\"\"Count the number of records in the DataFrame.\"\"\"\n    return len(data)\n\n# Create sample data\ndata = pd.DataFrame({\n    'category': ['A', 'B', 'A', 'C'],\n    'value': [10, 20, 30, 40]\n})\n\n# Define configuration\nconfig = {\n    'basic_count': {\n        'method': ['count_records'],\n        'filter': {}\n    }\n}\n\n# Generate metrics\nresults = generate_metrics(data, config)\nprint(results['basic_count']['count_records'])  # Should print: 4\n</code></pre> <p>If this example runs successfully, you're ready to explore the Usage Guide!</p>"},{"location":"getting_started/#development-setup","title":"Development Setup","text":"<p>For contributing to Quick Metric or working with the source code:</p> uv (Recommended)pip <pre><code>git clone https://github.com/nhsengland/quick_metric.git # (1)!\ncd quick_metric # (2)!\nuv venv # (3)!\nsource .venv/bin/activate # (4)!\nuv pip install -e \".[dev,docs]\" # (5)!\nuv pip install pre-commit # (6)!\nuv run pre-commit install # (7)!\n</code></pre> <ol> <li>Clone the repository from GitHub</li> <li>Navigate to the project directory</li> <li>Create a virtual environment using uv</li> <li>Activate the virtual environment (On Windows: <code>.venv\\Scripts\\activate</code>)</li> <li>Install in development mode with all dependencies</li> <li>Install pre-commit for code quality hooks</li> <li>Set up pre-commit hooks to run automatically on commits</li> </ol> <pre><code>git clone https://github.com/nhsengland/quick_metric.git # (1)!\ncd quick_metric # (2)!\npython -m venv .venv # (3)!\nsource .venv/bin/activate # (4)!\npip install -e \".[dev,docs]\" # (5)!\npip install pre-commit # (6)!\npre-commit install # (7)!\n</code></pre> <ol> <li>Clone the repository from GitHub</li> <li>Navigate to the project directory</li> <li>Create a virtual environment using Python's built-in venv</li> <li>Activate the virtual environment (On Windows: <code>.venv\\Scripts\\activate</code>)</li> <li>Install in development mode with all dependencies</li> <li>Install pre-commit for code quality hooks</li> <li>Set up pre-commit hooks to run automatically on commits</li> </ol> <p>Contribution Guidelines</p> <p>This project follows modern Python development practices:</p> <ul> <li>Linting &amp; Formatting: We use <code>ruff</code> for code formatting and linting</li> <li>Testing: <code>pytest</code> with structured unit and end-to-end tests</li> <li>Dependency Management: <code>uv</code> is preferred for faster dependency resolution</li> <li>Documentation: <code>mkdocs</code> with Material theme</li> <li>Docstrings: NumPy-style docstring format</li> </ul> <p>For detailed contribution guidelines, see our Contributing Guide.</p> <p>Pre-commit Hooks</p> <p>We strongly recommend setting up pre-commit hooks during development. These automatically run code quality checks before each commit, preventing issues early in the development process. Pre-commit hooks are included in both development setup methods above.</p>"},{"location":"getting_started/#development-workflow","title":"Development Workflow","text":""},{"location":"getting_started/#testing","title":"Testing","text":"<pre><code>uv run python -m pytest # (1)!\nuv run python -m pytest tests/unit/test_output_formats.py # (2)!\nuv run python -m pytest --cov=quick_metric # (3)!\nuv run python -m pytest --cov=quick_metric --cov-report=html # (4)!\n</code></pre> <ol> <li>Run all tests in the project</li> <li>Run a specific test file</li> <li>Run tests with coverage reporting</li> <li>Generate an HTML coverage report for detailed analysis</li> </ol>"},{"location":"getting_started/#code-quality","title":"Code Quality","text":"<pre><code>uv run ruff format quick_metric/ tests/ # (1)!\nuv run ruff check quick_metric/ tests/ # (2)!\nmake lint # (3)!\n</code></pre> <ol> <li>Format code automatically according to project standards</li> <li>Check for linting issues and code quality problems</li> <li>Run all quality checks using the Makefile (if available)</li> </ol>"},{"location":"getting_started/#documentation","title":"Documentation","text":"<pre><code>uv run mkdocs serve # (1)!\nuv run mkdocs build # (2)!\nuv run mkdocs gh-deploy # (3)!\n</code></pre> <ol> <li>Serve documentation locally with live reload for development</li> <li>Build static documentation files for deployment</li> <li>Deploy to GitHub Pages (maintainers only)</li> </ol>"},{"location":"getting_started/#next-steps","title":"Next Steps","text":"<p>Now that you have Quick Metric installed, explore these resources:</p> <ul> <li>Usage Guide - Comprehensive guide to all Quick Metric features</li> <li>API Reference - Detailed API documentation</li> <li>Configuration Guide - Learn how to write effective YAML configurations</li> </ul>"},{"location":"pipeline_integration/","title":"Pipeline Integration Guide","text":"<p>Learn how to integrate Quick Metric into data processing workflows using the <code>oops-its-a-pipeline</code> framework.</p>"},{"location":"pipeline_integration/#overview","title":"Overview","text":"<p>Quick Metric provides seamless integration with <code>oops-its-a-pipeline</code> for building complex data processing workflows. This allows you to:</p> <ul> <li>Chain metrics generation with data preprocessing steps</li> <li>Use metrics results in downstream pipeline stages</li> <li>Handle configuration and data dependencies automatically</li> <li>Scale metrics processing within larger workflows</li> </ul>"},{"location":"pipeline_integration/#basic-pipeline-usage","title":"Basic Pipeline Usage","text":""},{"location":"pipeline_integration/#simple-integration","title":"Simple Integration","text":"<pre><code>from oops_its_a_pipeline import Pipeline, PipelineConfig\nfrom quick_metric.pipeline import create_metrics_stage\n\nclass Config(PipelineConfig):\n    model_config = {'arbitrary_types_allowed': True}\n    data = your_dataframe\n    config = your_metrics_config\n\npipeline = Pipeline(Config()).add_stage(create_metrics_stage())\nresults = pipeline.run(\"analysis\")\n\n# Access metrics\nmetrics = results.context[\"metrics\"]\n</code></pre>"},{"location":"pipeline_integration/#advanced-pipeline-configuration","title":"Advanced Pipeline Configuration","text":"<pre><code># Multi-stage pipeline with data processing\npipeline = (Pipeline(config)\n    .add_function_stage(load_data, outputs=\"raw_data\")\n    .add_function_stage(clean_data, inputs=\"raw_data\", outputs=\"clean_data\")\n    .add_stage(create_metrics_stage(\n        data_input=\"clean_data\",\n        config_input=\"metrics_config\",\n        metrics_output=\"business_metrics\",\n        output_format=\"flat_dataframe\"\n    ))\n    .add_function_stage(save_results, inputs=\"business_metrics\"))\n\nresults = pipeline.run(\"full_analysis\")\n</code></pre>"},{"location":"pipeline_integration/#configuration-options","title":"Configuration Options","text":""},{"location":"pipeline_integration/#stage-parameters","title":"Stage Parameters","text":"<pre><code>create_metrics_stage(\n    data_input=\"processed_data\",      # Input DataFrame variable name\n    config_input=\"metrics_config\",    # Configuration variable name  \n    metrics_output=\"calculated_metrics\", # Output variable name\n    name=\"business_metrics\",          # Stage name for logging\n    output_format=\"dataframe\"         # Output format\n)\n</code></pre>"},{"location":"pipeline_integration/#using-yaml-configuration-files","title":"Using YAML Configuration Files","text":"<pre><code>stage = create_metrics_stage(\n    data_input=\"clean_data\",\n    config_file_path=\"config/pipeline_metrics.yaml\",\n    metrics_output=\"pipeline_metrics\"\n)\n</code></pre>"},{"location":"pipeline_integration/#error-handling","title":"Error Handling","text":"<p>Pipeline stages include comprehensive error handling:</p> <pre><code>try:\n    results = pipeline.run(\"analysis\")\nexcept Exception as e:\n    print(f\"Pipeline failed: {e}\")\n    # Detailed error information available\n</code></pre>"},{"location":"pipeline_integration/#best-practices","title":"Best Practices","text":""},{"location":"pipeline_integration/#stage-naming","title":"Stage Naming","text":"<p>Use descriptive names for pipeline stages to improve debugging and monitoring:</p> <pre><code>create_metrics_stage(\n    name=\"customer_acquisition_metrics\",\n    data_input=\"customer_data\",\n    config_input=\"acquisition_config\"\n)\n</code></pre>"},{"location":"pipeline_integration/#data-dependencies","title":"Data Dependencies","text":"<p>Clearly define input/output dependencies between stages:</p> <pre><code># Stage 1: Data preprocessing\npipeline.add_function_stage(preprocess_data, \n                           inputs=\"raw_data\", \n                           outputs=\"clean_data\")\n\n# Stage 2: Metrics calculation  \npipeline.add_stage(create_metrics_stage(\n    data_input=\"clean_data\",\n    metrics_output=\"metrics_results\"\n))\n\n# Stage 3: Results processing\npipeline.add_function_stage(analyze_metrics,\n                           inputs=\"metrics_results\",\n                           outputs=\"final_analysis\")\n</code></pre> <p>For complete pipeline documentation, see the oops-its-a-pipeline documentation.</p>"},{"location":"api_reference/","title":"API Reference","text":"<p>This section provides detailed API documentation for Quick Metric, covering core functions, method definitions, output formats, and pipeline integration.</p> <p>Quick Metric: A framework for quickly creating metrics using YAML configs.</p> <p>This package provides a simple way to apply filters and methods to pandas DataFrames based on YAML configuration files. It allows users to define custom metric methods and configure complex data filtering through declarative YAML configurations.</p> <p>The main workflow involves: 1. Define custom metric methods using the @metric_method decorator 2. Create YAML configurations specifying filters and methods to apply 3. Apply the configuration to pandas DataFrames to generate metrics</p> <p>Examples:</p> <p>Basic usage with generate_metrics:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from quick_metric import metric_method, generate_metrics\n&gt;&gt;&gt;\n&gt;&gt;&gt; @metric_method\n... def count_records(data):\n...     return len(data)\n&gt;&gt;&gt;\n&gt;&gt;&gt; @metric_method\n... def average_col(data, column='value'):\n...     return data[column].mean()\n&gt;&gt;&gt;\n&gt;&gt;&gt; data = pd.DataFrame({'category': ['A', 'B', 'A'], 'value': [1, 2, 3]})\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Simple method specification\n&gt;&gt;&gt; config1 = {\n...     'record_count': {\n...         'method': 'count_records',\n...         'filter': {}\n...     }\n... }\n&gt;&gt;&gt; results1 = generate_metrics(data, config1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Multiple methods\n&gt;&gt;&gt; config2 = {\n...     'analysis': {\n...         'method': ['count_records', 'average_col'],\n...         'filter': {'category': 'A'}\n...     }\n... }\n&gt;&gt;&gt; results2 = generate_metrics(data, config2)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Method with parameters\n&gt;&gt;&gt; config3 = {\n...     'custom_avg': {\n...         'method': {'average_col': {'column': 'value'}},\n...         'filter': {}\n...     }\n... }\n&gt;&gt;&gt; results3 = generate_metrics(data, config3)\n</code></pre> <p>Method discovery:</p> <pre><code>&gt;&gt;&gt; from quick_metric import metric_method\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Get information about a specific method\n&gt;&gt;&gt; method_info = metric_method('count_records')\n&gt;&gt;&gt;\n&gt;&gt;&gt; # List all available methods\n&gt;&gt;&gt; all_methods = metric_method()\n&gt;&gt;&gt; print(sorted(all_methods.keys()))\n</code></pre> Note <p>This module provides a clean, minimal public API focused on the most common use cases. Method specifications support multiple flexible formats: - Single method: \"method\": \"method_name\" - Multiple methods: \"method\": [\"method1\", \"method2\"] - Method with parameters: \"method\": {\"method_name\": {\"param\": \"value\"}}</p> See Also <p>method_definitions : Core decorator for registering metric methods core : Main metric generation functionality</p>"},{"location":"api_reference/#quick_metric.generate_metrics","title":"<code>generate_metrics(data, config, metrics_methods=None, output_format='nested')</code>","text":"<p>Generate metrics from data using configuration (main entry point).</p> <p>This is the primary entry point for the quick_metric framework. It provides a simple interface for generating metrics from pandas DataFrames using either YAML configuration files or dictionary configurations.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The DataFrame to process and generate metrics from.</p> required <code>config</code> <code>Path or Dict</code> <p>Either a Path object pointing to a YAML configuration file or a dictionary containing metric instructions. If a Path, the YAML file should contain a 'metric_instructions' key with the configuration.</p> required <code>metrics_methods</code> <code>Dict</code> <p>Dictionary of available methods. If None, uses the default registered methods from METRICS_METHODS.</p> <code>None</code> <code>output_format</code> <code>str or OutputFormat</code> <p>Format for the output. Options: - \"nested\": Current dict of dicts format {'metric': {'method': result}} - \"dataframe\": Pandas DataFrame with columns [metric, method, value, value_type] - \"records\": List of dicts [{'metric': '...', 'method': '...', 'value': ...}]</p> <code>\"nested\"</code> <p>Returns:</p> Type Description <code>Union[dict, DataFrame, list[dict]]</code> <p>Results in the specified format. The exact type depends on output_format: - dict: When output_format=\"nested\" (default) - pd.DataFrame: When output_format=\"dataframe\" - list[dict]: When output_format=\"records\"</p> <p>Examples:</p> <p>Using a dictionary configuration (default nested format):</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from quick_metric import generate_metrics, metric_method\n&gt;&gt;&gt;\n&gt;&gt;&gt; @metric_method\n... def count_records(data):\n...     return len(data)\n&gt;&gt;&gt;\n&gt;&gt;&gt; data = pd.DataFrame({'category': ['A', 'B', 'A'], 'value': [1, 2, 3]})\n&gt;&gt;&gt; config = {\n...     'category_a_count': {\n...         'method': ['count_records'],\n...         'filter': {'category': 'A'}\n...     }\n... }\n&gt;&gt;&gt; results = generate_metrics(data, config)\n&gt;&gt;&gt; # Returns: {'category_a_count': {'count_records': 2}}\n</code></pre> <p>Using DataFrame output format:</p> <pre><code>&gt;&gt;&gt; df_results = generate_metrics(data, config, output_format=\"dataframe\")\n&gt;&gt;&gt; # Returns: DataFrame with columns [metric, method, value, value_type]\n</code></pre> <p>Using records output format:</p> <pre><code>&gt;&gt;&gt; records = generate_metrics(data, config, output_format=\"records\")\n&gt;&gt;&gt; # Returns: [{'metric': 'category_a_count', 'method': 'count_records', 'value': 2}]\n</code></pre> <p>Using a YAML file:</p> <pre><code>&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; config_path = Path('my_metrics.yaml')\n&gt;&gt;&gt; results = generate_metrics(data, config_path)\n</code></pre> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the config path does not exist.</p> <code>KeyError</code> <p>If a YAML file doesn't contain 'metric_instructions' key.</p> <code>ValueError</code> <p>If config parameter or output_format is not a valid type.</p> Source code in <code>quick_metric/_core.py</code> <pre><code>def generate_metrics(\n    data: pd.DataFrame,\n    config: Union[Path, dict],\n    metrics_methods: Optional[dict] = None,\n    output_format: Union[str, OutputFormat] = \"nested\",\n) -&gt; Union[dict, pd.DataFrame, list[dict]]:\n    \"\"\"\n    Generate metrics from data using configuration (main entry point).\n\n    This is the primary entry point for the quick_metric framework. It provides\n    a simple interface for generating metrics from pandas DataFrames using\n    either YAML configuration files or dictionary configurations.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process and generate metrics from.\n    config : Path or Dict\n        Either a Path object pointing to a YAML configuration file or a\n        dictionary containing metric instructions. If a Path, the YAML file\n        should contain a 'metric_instructions' key with the configuration.\n    metrics_methods : Dict, optional\n        Dictionary of available methods. If None, uses the default registered\n        methods from METRICS_METHODS.\n    output_format : str or OutputFormat, default \"nested\"\n        Format for the output. Options:\n        - \"nested\": Current dict of dicts format {'metric': {'method': result}}\n        - \"dataframe\": Pandas DataFrame with columns [metric, method, value, value_type]\n        - \"records\": List of dicts [{'metric': '...', 'method': '...', 'value': ...}]\n\n    Returns\n    -------\n    Union[dict, pd.DataFrame, list[dict]]\n        Results in the specified format. The exact type depends on output_format:\n        - dict: When output_format=\"nested\" (default)\n        - pd.DataFrame: When output_format=\"dataframe\"\n        - list[dict]: When output_format=\"records\"\n\n    Examples\n    --------\n    Using a dictionary configuration (default nested format):\n\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; from quick_metric import generate_metrics, metric_method\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; @metric_method\n    ... def count_records(data):\n    ...     return len(data)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; data = pd.DataFrame({'category': ['A', 'B', 'A'], 'value': [1, 2, 3]})\n    &gt;&gt;&gt; config = {\n    ...     'category_a_count': {\n    ...         'method': ['count_records'],\n    ...         'filter': {'category': 'A'}\n    ...     }\n    ... }\n    &gt;&gt;&gt; results = generate_metrics(data, config)\n    &gt;&gt;&gt; # Returns: {'category_a_count': {'count_records': 2}}\n\n    Using DataFrame output format:\n\n    &gt;&gt;&gt; df_results = generate_metrics(data, config, output_format=\"dataframe\")\n    &gt;&gt;&gt; # Returns: DataFrame with columns [metric, method, value, value_type]\n\n    Using records output format:\n\n    &gt;&gt;&gt; records = generate_metrics(data, config, output_format=\"records\")\n    &gt;&gt;&gt; # Returns: [{'metric': 'category_a_count', 'method': 'count_records', 'value': 2}]\n\n    Using a YAML file:\n\n    &gt;&gt;&gt; from pathlib import Path\n    &gt;&gt;&gt; config_path = Path('my_metrics.yaml')\n    &gt;&gt;&gt; results = generate_metrics(data, config_path)\n\n    Raises\n    ------\n    FileNotFoundError\n        If the config path does not exist.\n    KeyError\n        If a YAML file doesn't contain 'metric_instructions' key.\n    ValueError\n        If config parameter or output_format is not a valid type.\n    \"\"\"\n    logger.info(\"Starting metric generation\")\n\n    # Convert string format to enum\n    if isinstance(output_format, str):\n        try:\n            output_format = OutputFormat(output_format)\n        except ValueError as e:\n            valid_formats = [f.value for f in OutputFormat]\n            raise ValueError(\n                f\"Invalid output_format '{output_format}'. Valid options: {valid_formats}\"\n            ) from e\n\n    # Handle different config input types\n    if isinstance(config, Path):\n        logger.debug(f\"Loading configuration from file: {config}\")\n        metric_instructions = read_metric_instructions(config)\n    elif isinstance(config, dict):\n        logger.debug(\"Using provided dictionary configuration\")\n        metric_instructions = config\n    else:\n        logger.error(f\"Invalid config type: {type(config)}\")\n        raise ValueError(f\"Config must be a pathlib.Path object or dict, got {type(config)}\")\n\n    # Generate metrics using the existing function\n    results = interpret_metric_instructions(\n        data=data,\n        metric_instructions=metric_instructions,\n        metrics_methods=metrics_methods,\n    )\n\n    # Convert to requested format\n    if output_format != OutputFormat.NESTED:\n        logger.debug(f\"Converting results to {output_format.value} format\")\n        results = convert_to_format(results, output_format)\n\n    logger.success(\"Metric generation completed successfully\")\n    return results\n</code></pre>"},{"location":"api_reference/#quick_metric.clear_methods","title":"<code>clear_methods()</code>","text":"<p>Clear all registered methods (for testing).</p> Source code in <code>quick_metric/_method_definitions.py</code> <pre><code>def clear_methods() -&gt; None:\n    \"\"\"Clear all registered methods (for testing).\"\"\"\n    _registry.clear()\n</code></pre>"},{"location":"api_reference/#quick_metric.get_method","title":"<code>get_method(name)</code>","text":"<p>Get a registered method by name.</p> Source code in <code>quick_metric/_method_definitions.py</code> <pre><code>def get_method(name: str) -&gt; Callable:\n    \"\"\"Get a registered method by name.\"\"\"\n    return _registry.get_method(name)\n</code></pre>"},{"location":"api_reference/#quick_metric.get_registered_methods","title":"<code>get_registered_methods()</code>","text":"<p>Get all registered methods.</p> Source code in <code>quick_metric/_method_definitions.py</code> <pre><code>def get_registered_methods() -&gt; dict[str, Callable]:\n    \"\"\"Get all registered methods.\"\"\"\n    return _registry.get_methods()\n</code></pre>"},{"location":"api_reference/#quick_metric.list_method_names","title":"<code>list_method_names()</code>","text":"<p>Get list of registered method names.</p> Source code in <code>quick_metric/_method_definitions.py</code> <pre><code>def list_method_names() -&gt; list[str]:\n    \"\"\"Get list of registered method names.\"\"\"\n    return _registry.list_method_names()\n</code></pre>"},{"location":"api_reference/#quick_metric.metric_method","title":"<code>metric_method(func_or_name=None)</code>","text":"<p>Decorator to register a user function as a metric method, or query registered methods.</p> <p>Can be used in three ways: 1. As a decorator: @metric_method 2. To get all methods: metric_method() 3. To get a specific method: metric_method('method_name')</p> <p>Parameters:</p> Name Type Description Default <code>func_or_name</code> <code>Callable or str</code> <p>User function to register when used as decorator, or method name to retrieve.</p> <code>None</code> <p>Returns:</p> Type Description <code>Callable or dict</code> <p>When used as decorator: returns the original function unchanged. When called without args: returns dict of all registered methods. When called with method name: returns the specific method.</p> <p>Examples:</p> <p>As a decorator:</p> <pre><code>&gt;&gt;&gt; @metric_method\n... def my_custom_metric(data):\n...     return len(data)\n</code></pre> <p>To get all methods:</p> <pre><code>&gt;&gt;&gt; all_methods = metric_method()\n&gt;&gt;&gt; print(list(all_methods.keys()))\n</code></pre> <p>To get a specific method:</p> <pre><code>&gt;&gt;&gt; my_method = metric_method('my_custom_metric')\n</code></pre> Source code in <code>quick_metric/_method_definitions.py</code> <pre><code>def metric_method(func_or_name=None):\n    \"\"\"\n    Decorator to register a user function as a metric method, or query registered methods.\n\n    Can be used in three ways:\n    1. As a decorator: @metric_method\n    2. To get all methods: metric_method()\n    3. To get a specific method: metric_method('method_name')\n\n    Parameters\n    ----------\n    func_or_name : Callable or str, optional\n        User function to register when used as decorator, or method name to retrieve.\n\n    Returns\n    -------\n    Callable or dict\n        When used as decorator: returns the original function unchanged.\n        When called without args: returns dict of all registered methods.\n        When called with method name: returns the specific method.\n\n    Examples\n    --------\n    As a decorator:\n    &gt;&gt;&gt; @metric_method\n    ... def my_custom_metric(data):\n    ...     return len(data)\n\n    To get all methods:\n    &gt;&gt;&gt; all_methods = metric_method()\n    &gt;&gt;&gt; print(list(all_methods.keys()))\n\n    To get a specific method:\n    &gt;&gt;&gt; my_method = metric_method('my_custom_metric')\n    \"\"\"\n    # Case 1: Called without arguments - return all methods\n    if func_or_name is None:\n        return _registry.get_methods()\n\n    # Case 2: Called with a string - return specific method\n    if isinstance(func_or_name, str):\n        return _registry.get_method(func_or_name)\n\n    # Case 3: Called with a function (decorator usage)\n    if callable(func_or_name):\n        return _registry.register(func_or_name)\n\n    raise ValueError(f\"Invalid argument type: {type(func_or_name)}\")\n</code></pre>"},{"location":"api_reference/apply_methods/","title":"Apply Methods","text":"<p>Method application and execution functionality for Quick Metric.</p> <p>This module handles the application of registered metric methods to filtered pandas DataFrames. It provides both single method application and batch method application capabilities, with comprehensive error handling for missing methods.</p> <p>The module acts as the execution engine for the quick_metric framework, taking registered methods from the METRICS_METHODS registry and applying them to data with proper error handling and result collection.</p> <p>Functions:</p> Name Description <code>apply_method : Apply a single metric method to data</code> <code>apply_methods : Apply multiple metric methods to data and collect results</code> Exceptions <p>MetricsMethodNotFoundError : Raised when a requested method is not registered</p> <p>Examples:</p> <p>Apply a single method:</p> <pre><code>Examples:\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; from quick_metric._method_definitions import metric_method\n    &gt;&gt;&gt; from quick_metric._apply_methods import apply_method\n</code></pre> <pre><code>&gt;&gt;&gt;\n&gt;&gt;&gt; @metric_method\n... def count_rows(data):\n...     return len(data)\n&gt;&gt;&gt;\n&gt;&gt;&gt; data = pd.DataFrame({'a': [1, 2, 3]})\n&gt;&gt;&gt; result = apply_method('count_rows', data)\n&gt;&gt;&gt; print(result)\n3\n</code></pre> <p>Apply multiple methods:</p> <pre><code>&gt;&gt;&gt; from quick_metric._apply_methods import apply_methods\n</code></pre> <pre><code>&gt;&gt;&gt;\n&gt;&gt;&gt; @metric_method\n... def sum_column(data, column='a'):\n...     return data[column].sum()\n&gt;&gt;&gt;\n&gt;&gt;&gt; methods = ['count_rows', 'sum_column']\n&gt;&gt;&gt; results = apply_methods(methods, data)\n&gt;&gt;&gt; print(results)\n{'count_rows': 3, 'sum_column': 6}\n</code></pre> See Also <p>_method_definitions : Module for registering methods _filter : Module for data filtering before method application</p>"},{"location":"api_reference/apply_methods/#quick_metric._apply_methods.apply_method","title":"<code>apply_method(data, method_spec, metrics_methods=None)</code>","text":"<p>Apply the specified method to the filtered data from metrics methods.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The DataFrame containing the filtered data.</p> required <code>method_spec</code> <code>str or dict</code> <p>The method specification. Can be either: - str: The name of the method to be applied - dict: A dictionary with method name as key and parameters as value        e.g., {'method_name': {'param1': value1, 'param2': value2}}</p> required <code>metrics_methods</code> <code>Dict[str, Callable]</code> <p>A dictionary of metrics methods. Defaults to METRICS_METHODS. This dictionary maps method names to their corresponding functions.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[str, Any]</code> <p>A tuple containing (result_key, result) where result_key is the key to use in the results dictionary and result is the method output.</p> <p>Raises:</p> Type Description <code>MetricsMethodNotFoundError</code> <p>If the specified method is not found in the metrics methods.</p> Source code in <code>quick_metric/_apply_methods.py</code> <pre><code>def apply_method(\n    data: pd.DataFrame,\n    method_spec: str | dict,\n    metrics_methods: Optional[dict[str, Callable]] = None,\n) -&gt; tuple[str, Any]:\n    \"\"\"\n    Apply the specified method to the filtered data from metrics methods.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame containing the filtered data.\n    method_spec : str or dict\n        The method specification. Can be either:\n        - str: The name of the method to be applied\n        - dict: A dictionary with method name as key and parameters as value\n               e.g., {'method_name': {'param1': value1, 'param2': value2}}\n    metrics_methods : Dict[str, Callable], optional\n        A dictionary of metrics methods. Defaults to METRICS_METHODS.\n        This dictionary maps method names to their corresponding functions.\n\n    Returns\n    -------\n    tuple[str, Any]\n        A tuple containing (result_key, result) where result_key is the key\n        to use in the results dictionary and result is the method output.\n\n    Raises\n    -------\n    MetricsMethodNotFoundError\n        If the specified method is not found in the metrics methods.\n    \"\"\"\n    if not metrics_methods:\n        metrics_methods = METRICS_METHODS\n\n    # Parse method specification\n    if isinstance(method_spec, str):\n        method_name = method_spec\n        method_params = {}\n        result_key = method_name\n    elif isinstance(method_spec, dict):\n        if len(method_spec) != 1:\n            raise MetricSpecificationError(\n                f\"Method specification must contain exactly one method, got: {method_spec}\",\n                method_spec,\n            )\n        method_name, method_params = next(iter(method_spec.items()))\n        if not isinstance(method_params, dict):\n            raise MetricSpecificationError(\n                f\"Method parameters must be a dictionary, got: {type(method_params)}\", method_spec\n            )\n        # Create a result key that includes parameters for uniqueness\n        if method_params:\n            # For complex parameters, use a hash to keep names manageable\n            param_repr = str(sorted(method_params.items()))\n            if len(param_repr) &gt; 50:  # If parameter representation is too long\n                param_hash = hashlib.md5(param_repr.encode()).hexdigest()[:8]\n                result_key = f\"{method_name}_{param_hash}\"\n            else:\n                # For simple parameters, use readable format\n                param_str = \"_\".join(f\"{k}{v}\" for k, v in sorted(method_params.items()))\n                result_key = f\"{method_name}_{param_str}\"\n        else:\n            result_key = method_name\n    else:\n        raise MetricSpecificationError(\n            f\"Method specification must be str or dict, got: {type(method_spec)}\", method_spec\n        )\n\n    logger.trace(f\"Applying method '{method_name}' with params {method_params} to {len(data)} rows\")\n\n    try:\n        method = metrics_methods[method_name]\n    except KeyError as e:\n        logger.error(f\"Method '{method_name}' not found in available methods\")\n        raise MetricsMethodNotFoundError(method_name, list(metrics_methods.keys())) from e\n\n    try:\n        # Call method with parameters\n        result = method(data, **method_params) if method_params else method(data)\n        logger.success(f\"Method '{method_name}' completed successfully\")\n        return result_key, result\n    except Exception as e:\n        logger.critical(f\"Error applying method '{method_name}': {e}\")\n        raise\n</code></pre>"},{"location":"api_reference/apply_methods/#quick_metric._apply_methods.apply_methods","title":"<code>apply_methods(data, method_specs, metrics_methods=None)</code>","text":"<p>Apply multiple methods to the data. The methods are specified by their names or as parameter dictionaries and are looked up in the metrics_methods dictionary. The results are returned in a dictionary where the keys are method names (potentially with parameters) and the values are the results of applying the methods.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The DataFrame containing the filtered data.</p> required <code>method_specs</code> <code>List[str | dict]</code> <p>A list of method specifications. Each can be either: - str: The name of the method to be applied - dict: A dictionary with method name as key and parameters as value</p> required <code>metrics_methods</code> <code>Dict[str, Callable]</code> <p>A dictionary of metrics methods. Defaults to METRICS_METHODS. This dictionary maps method names to their corresponding functions.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary where the keys are the method names (with parameters if any) and the values are the results of applying the methods.</p> Source code in <code>quick_metric/_apply_methods.py</code> <pre><code>def apply_methods(\n    data: pd.DataFrame,\n    method_specs: list[str | dict],\n    metrics_methods: Optional[dict[str, Callable]] = None,\n) -&gt; dict[str, Any]:\n    \"\"\"\n    Apply multiple methods to the data.\n    The methods are specified by their names or as parameter dictionaries and are\n    looked up in the metrics_methods dictionary.\n    The results are returned in a dictionary where the keys are method names\n    (potentially with parameters) and the values are the results of applying the methods.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame containing the filtered data.\n    method_specs : List[str | dict]\n        A list of method specifications. Each can be either:\n        - str: The name of the method to be applied\n        - dict: A dictionary with method name as key and parameters as value\n    metrics_methods : Dict[str, Callable], optional\n        A dictionary of metrics methods. Defaults to METRICS_METHODS.\n        This dictionary maps method names to their corresponding functions.\n\n    Returns\n    -------\n    Dict[str, Any]\n        A dictionary where the keys are the method names (with parameters if any)\n        and the values are the results of applying the methods.\n    \"\"\"\n    if not metrics_methods:\n        metrics_methods = METRICS_METHODS\n\n    logger.debug(f\"Applying {len(method_specs)} methods: {method_specs}\")\n\n    results = {}\n    for method_spec in method_specs:\n        result_key, result_value = apply_method(data, method_spec, metrics_methods)\n        results[result_key] = result_value\n\n    logger.success(f\"Successfully applied all {len(method_specs)} methods\")\n    return results\n</code></pre>"},{"location":"api_reference/core/","title":"Core Functions","text":"<p>Main orchestration and workflow functionality for Quick Metric.</p> <p>This module provides the primary entry points for the quick_metric framework, handling the complete workflow from YAML configuration reading to metric result generation. It coordinates between the filtering, method application, and configuration parsing components.</p> <p>The module serves as the main interface for users, providing high-level functions that abstract away the complexity of the underlying filtering and method application processes.</p> <p>Functions:</p> Name Description <code>read_metric_instructions : Load metric configurations from YAML files</code> <code>interpret_metric_instructions : Execute complete metric workflow on data</code> Workflow <ol> <li>Load YAML configuration specifying metrics, filters, and methods</li> <li>For each metric specification:</li> <li>Apply filters to subset the input DataFrame</li> <li>Execute specified methods on the filtered data</li> <li>Collect results in a structured dictionary</li> <li>Return comprehensive results for all metrics</li> </ol> <p>Examples:</p> <p>Load configuration from YAML file:</p> <pre><code>&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; from quick_metric._core import read_metric_instructions\n&gt;&gt;&gt;\n&gt;&gt;&gt; config_path = Path('metrics.yaml')\n&gt;&gt;&gt; instructions = read_metric_instructions(config_path)\n</code></pre> <p>Execute complete workflow:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from quick_metric._core import interpret_metric_instructions\n&gt;&gt;&gt; from quick_metric._method_definitions import metric_method\n&gt;&gt;&gt;\n&gt;&gt;&gt; @metric_method\n... def count_records(data):\n...     return len(data)\n&gt;&gt;&gt;\n&gt;&gt;&gt; data = pd.DataFrame({'category': ['A', 'B', 'A'], 'value': [1, 2, 3]})\n&gt;&gt;&gt; config = {\n...     'category_metrics': {\n...         'method': ['count_records'],\n...         'filter': {'category': 'A'}\n...     }\n... }\n&gt;&gt;&gt; results = interpret_metric_instructions(data, config)\n&gt;&gt;&gt; print(results['category_metrics']['count_records'])\n2\n</code></pre> YAML Configuration Format <pre><code>metric_instructions:\n  metric_name:\n    method: ['method1', 'method2']\n    filter:\n      column_name: value\n      and:\n        condition1: value1\n        condition2: value2\n</code></pre> See Also <p>filter : Data filtering functionality used by this module apply_methods : Method execution functionality used by this module method_definitions : Method registration system used by this module</p>"},{"location":"api_reference/core/#quick_metric._core.read_metric_instructions","title":"<code>read_metric_instructions(metric_config_path)</code>","text":"<p>Read metric_instructions dictionary from a YAML config file.</p> <p>Parameters:</p> Name Type Description Default <code>metric_config_path</code> <code>Path</code> <p>Path to the YAML config file containing metric instructions.</p> required <p>Returns:</p> Type Description <code>Dict</code> <p>The 'metric_instructions' dictionary from the YAML file.</p> Source code in <code>quick_metric/_core.py</code> <pre><code>def read_metric_instructions(metric_config_path: Path) -&gt; dict:\n    \"\"\"\n    Read metric_instructions dictionary from a YAML config file.\n\n    Parameters\n    ----------\n    metric_config_path : Path\n        Path to the YAML config file containing metric instructions.\n\n    Returns\n    -------\n    Dict\n        The 'metric_instructions' dictionary from the YAML file.\n    \"\"\"\n    logger.info(f\"Reading metric configuration from {metric_config_path}\")\n\n    if not metric_config_path.exists():\n        logger.error(f\"Configuration file not found: {metric_config_path}\")\n        raise FileNotFoundError(f\"Configuration file not found: {metric_config_path}\")\n\n    try:\n        with open(metric_config_path, encoding=\"utf-8\") as file:\n            metric_configs = yaml.safe_load(file)\n\n        if not isinstance(metric_configs, dict):\n            logger.error(\"Configuration file must contain a YAML dictionary\")\n            raise ValueError(\"Configuration file must contain a YAML dictionary\")\n\n        metric_instructions = metric_configs.get(\"metric_instructions\", {})\n\n        if not metric_instructions:\n            logger.warning(\"No 'metric_instructions' found in configuration file\")\n        else:\n            logger.success(f\"Loaded {len(metric_instructions)} metric configurations\")\n\n        return metric_instructions\n\n    except yaml.YAMLError as e:\n        logger.error(f\"Invalid YAML in configuration file: {e}\")\n        raise ValueError(f\"Invalid YAML in configuration file: {e}\") from e\n</code></pre>"},{"location":"api_reference/core/#quick_metric._core.interpret_metric_instructions","title":"<code>interpret_metric_instructions(data, metric_instructions, metrics_methods=None)</code>","text":"<p>Apply filters and methods from metric instructions to a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The DataFrame to be processed.</p> required <code>metric_instructions</code> <code>Dict</code> <p>Dictionary containing the metrics and their filter/method conditions.</p> required <code>metrics_methods</code> <code>Dict</code> <p>Dictionary of available methods. Defaults to METRICS_METHODS.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict</code> <p>Dictionary with metric names as keys and method results as values.</p> Source code in <code>quick_metric/_core.py</code> <pre><code>def interpret_metric_instructions(\n    data: pd.DataFrame,\n    metric_instructions: dict,\n    metrics_methods: Optional[dict] = None,\n) -&gt; dict:\n    \"\"\"\n    Apply filters and methods from metric instructions to a DataFrame.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to be processed.\n    metric_instructions : Dict\n        Dictionary containing the metrics and their filter/method conditions.\n    metrics_methods : Dict, optional\n        Dictionary of available methods. Defaults to METRICS_METHODS.\n\n    Returns\n    -------\n    Dict\n        Dictionary with metric names as keys and method results as values.\n    \"\"\"\n    if metrics_methods is None:\n        metrics_methods = METRICS_METHODS\n\n    logger.info(f\"Processing {len(metric_instructions)} metrics on DataFrame with {len(data)} rows\")\n\n    # Basic validation\n    if not isinstance(metric_instructions, dict):\n        logger.error(\"metric_instructions must be a dictionary\")\n        raise ValueError(\"metric_instructions must be a dictionary\")\n\n    if data.empty:\n        logger.warning(\"Input DataFrame is empty\")\n\n    results = {}\n\n    for metric_name, metric_instruction in metric_instructions.items():\n        with logger.contextualize(metric=metric_name):\n            logger.trace(\"Processing metric\")\n\n            # Validate metric instruction structure\n            if not isinstance(metric_instruction, dict):\n                logger.error(\"Metric instruction must be a dict\")\n                raise ValueError(f\"Metric '{metric_name}' instruction must be a dictionary\")\n\n            if \"method\" not in metric_instruction:\n                logger.error(\"Metric missing 'method' key\")\n                raise ValueError(f\"Metric '{metric_name}' missing required 'method' key\")\n\n            if \"filter\" not in metric_instruction:\n                logger.error(\"Metric missing 'filter' key\")\n                raise ValueError(f\"Metric '{metric_name}' missing required 'filter' key\")\n\n            # Apply filter to data\n            filtered_data = apply_filter(data_df=data, filters=metric_instruction[\"filter\"])\n\n            logger.trace(f\"Filtered to {len(filtered_data)} rows\")\n\n            # Normalize method specifications to handle various input formats\n            normalized_methods = _normalize_method_specs(metric_instruction[\"method\"])\n\n            # Apply methods to filtered data\n            with logger.contextualize(methods=normalized_methods):\n                results[metric_name] = apply_methods(\n                    data=filtered_data,\n                    method_specs=normalized_methods,\n                    metrics_methods=metrics_methods,\n                )\n\n            logger.success(\"Metric completed successfully\")\n\n    logger.success(f\"Successfully processed all {len(results)} metrics\")\n    return results\n</code></pre>"},{"location":"api_reference/core/#quick_metric._core.generate_metrics","title":"<code>generate_metrics(data, config, metrics_methods=None, output_format='nested')</code>","text":"<p>Generate metrics from data using configuration (main entry point).</p> <p>This is the primary entry point for the quick_metric framework. It provides a simple interface for generating metrics from pandas DataFrames using either YAML configuration files or dictionary configurations.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The DataFrame to process and generate metrics from.</p> required <code>config</code> <code>Path or Dict</code> <p>Either a Path object pointing to a YAML configuration file or a dictionary containing metric instructions. If a Path, the YAML file should contain a 'metric_instructions' key with the configuration.</p> required <code>metrics_methods</code> <code>Dict</code> <p>Dictionary of available methods. If None, uses the default registered methods from METRICS_METHODS.</p> <code>None</code> <code>output_format</code> <code>str or OutputFormat</code> <p>Format for the output. Options: - \"nested\": Current dict of dicts format {'metric': {'method': result}} - \"dataframe\": Pandas DataFrame with columns [metric, method, value, value_type] - \"records\": List of dicts [{'metric': '...', 'method': '...', 'value': ...}]</p> <code>\"nested\"</code> <p>Returns:</p> Type Description <code>Union[dict, DataFrame, list[dict]]</code> <p>Results in the specified format. The exact type depends on output_format: - dict: When output_format=\"nested\" (default) - pd.DataFrame: When output_format=\"dataframe\" - list[dict]: When output_format=\"records\"</p> <p>Examples:</p> <p>Using a dictionary configuration (default nested format):</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from quick_metric import generate_metrics, metric_method\n&gt;&gt;&gt;\n&gt;&gt;&gt; @metric_method\n... def count_records(data):\n...     return len(data)\n&gt;&gt;&gt;\n&gt;&gt;&gt; data = pd.DataFrame({'category': ['A', 'B', 'A'], 'value': [1, 2, 3]})\n&gt;&gt;&gt; config = {\n...     'category_a_count': {\n...         'method': ['count_records'],\n...         'filter': {'category': 'A'}\n...     }\n... }\n&gt;&gt;&gt; results = generate_metrics(data, config)\n&gt;&gt;&gt; # Returns: {'category_a_count': {'count_records': 2}}\n</code></pre> <p>Using DataFrame output format:</p> <pre><code>&gt;&gt;&gt; df_results = generate_metrics(data, config, output_format=\"dataframe\")\n&gt;&gt;&gt; # Returns: DataFrame with columns [metric, method, value, value_type]\n</code></pre> <p>Using records output format:</p> <pre><code>&gt;&gt;&gt; records = generate_metrics(data, config, output_format=\"records\")\n&gt;&gt;&gt; # Returns: [{'metric': 'category_a_count', 'method': 'count_records', 'value': 2}]\n</code></pre> <p>Using a YAML file:</p> <pre><code>&gt;&gt;&gt; from pathlib import Path\n&gt;&gt;&gt; config_path = Path('my_metrics.yaml')\n&gt;&gt;&gt; results = generate_metrics(data, config_path)\n</code></pre> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the config path does not exist.</p> <code>KeyError</code> <p>If a YAML file doesn't contain 'metric_instructions' key.</p> <code>ValueError</code> <p>If config parameter or output_format is not a valid type.</p> Source code in <code>quick_metric/_core.py</code> <pre><code>def generate_metrics(\n    data: pd.DataFrame,\n    config: Union[Path, dict],\n    metrics_methods: Optional[dict] = None,\n    output_format: Union[str, OutputFormat] = \"nested\",\n) -&gt; Union[dict, pd.DataFrame, list[dict]]:\n    \"\"\"\n    Generate metrics from data using configuration (main entry point).\n\n    This is the primary entry point for the quick_metric framework. It provides\n    a simple interface for generating metrics from pandas DataFrames using\n    either YAML configuration files or dictionary configurations.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        The DataFrame to process and generate metrics from.\n    config : Path or Dict\n        Either a Path object pointing to a YAML configuration file or a\n        dictionary containing metric instructions. If a Path, the YAML file\n        should contain a 'metric_instructions' key with the configuration.\n    metrics_methods : Dict, optional\n        Dictionary of available methods. If None, uses the default registered\n        methods from METRICS_METHODS.\n    output_format : str or OutputFormat, default \"nested\"\n        Format for the output. Options:\n        - \"nested\": Current dict of dicts format {'metric': {'method': result}}\n        - \"dataframe\": Pandas DataFrame with columns [metric, method, value, value_type]\n        - \"records\": List of dicts [{'metric': '...', 'method': '...', 'value': ...}]\n\n    Returns\n    -------\n    Union[dict, pd.DataFrame, list[dict]]\n        Results in the specified format. The exact type depends on output_format:\n        - dict: When output_format=\"nested\" (default)\n        - pd.DataFrame: When output_format=\"dataframe\"\n        - list[dict]: When output_format=\"records\"\n\n    Examples\n    --------\n    Using a dictionary configuration (default nested format):\n\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; from quick_metric import generate_metrics, metric_method\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; @metric_method\n    ... def count_records(data):\n    ...     return len(data)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; data = pd.DataFrame({'category': ['A', 'B', 'A'], 'value': [1, 2, 3]})\n    &gt;&gt;&gt; config = {\n    ...     'category_a_count': {\n    ...         'method': ['count_records'],\n    ...         'filter': {'category': 'A'}\n    ...     }\n    ... }\n    &gt;&gt;&gt; results = generate_metrics(data, config)\n    &gt;&gt;&gt; # Returns: {'category_a_count': {'count_records': 2}}\n\n    Using DataFrame output format:\n\n    &gt;&gt;&gt; df_results = generate_metrics(data, config, output_format=\"dataframe\")\n    &gt;&gt;&gt; # Returns: DataFrame with columns [metric, method, value, value_type]\n\n    Using records output format:\n\n    &gt;&gt;&gt; records = generate_metrics(data, config, output_format=\"records\")\n    &gt;&gt;&gt; # Returns: [{'metric': 'category_a_count', 'method': 'count_records', 'value': 2}]\n\n    Using a YAML file:\n\n    &gt;&gt;&gt; from pathlib import Path\n    &gt;&gt;&gt; config_path = Path('my_metrics.yaml')\n    &gt;&gt;&gt; results = generate_metrics(data, config_path)\n\n    Raises\n    ------\n    FileNotFoundError\n        If the config path does not exist.\n    KeyError\n        If a YAML file doesn't contain 'metric_instructions' key.\n    ValueError\n        If config parameter or output_format is not a valid type.\n    \"\"\"\n    logger.info(\"Starting metric generation\")\n\n    # Convert string format to enum\n    if isinstance(output_format, str):\n        try:\n            output_format = OutputFormat(output_format)\n        except ValueError as e:\n            valid_formats = [f.value for f in OutputFormat]\n            raise ValueError(\n                f\"Invalid output_format '{output_format}'. Valid options: {valid_formats}\"\n            ) from e\n\n    # Handle different config input types\n    if isinstance(config, Path):\n        logger.debug(f\"Loading configuration from file: {config}\")\n        metric_instructions = read_metric_instructions(config)\n    elif isinstance(config, dict):\n        logger.debug(\"Using provided dictionary configuration\")\n        metric_instructions = config\n    else:\n        logger.error(f\"Invalid config type: {type(config)}\")\n        raise ValueError(f\"Config must be a pathlib.Path object or dict, got {type(config)}\")\n\n    # Generate metrics using the existing function\n    results = interpret_metric_instructions(\n        data=data,\n        metric_instructions=metric_instructions,\n        metrics_methods=metrics_methods,\n    )\n\n    # Convert to requested format\n    if output_format != OutputFormat.NESTED:\n        logger.debug(f\"Converting results to {output_format.value} format\")\n        results = convert_to_format(results, output_format)\n\n    logger.success(\"Metric generation completed successfully\")\n    return results\n</code></pre>"},{"location":"api_reference/exceptions/","title":"Exceptions","text":"<p>Custom exceptions for Quick Metric framework.</p> <p>This module provides domain-specific exceptions that inherit from nhs_herbot.LoggedException to provide clear, actionable error messages with automatic logging.</p>"},{"location":"api_reference/exceptions/#quick_metric._exceptions.MetricMethodError","title":"<code>MetricMethodError</code>","text":"<p>               Bases: <code>LoggedException</code></p> <p>Base exception for all metric method related errors.</p> Source code in <code>quick_metric/_exceptions.py</code> <pre><code>class MetricMethodError(LoggedException):\n    \"\"\"Base exception for all metric method related errors.\"\"\"\n</code></pre>"},{"location":"api_reference/exceptions/#quick_metric._exceptions.MethodRegistrationError","title":"<code>MethodRegistrationError</code>","text":"<p>               Bases: <code>MetricMethodError</code></p> <p>Exception raised when method registration fails.</p> Source code in <code>quick_metric/_exceptions.py</code> <pre><code>class MethodRegistrationError(MetricMethodError):\n    \"\"\"Exception raised when method registration fails.\"\"\"\n\n    def __init__(\n        self,\n        method_name: str,\n        reason: str,\n        existing_methods: Optional[list[str]] = None,\n    ):\n        self.method_name = method_name\n        self.reason = reason\n        self.existing_methods = existing_methods or []\n\n        message = f\"Failed to register metric method '{method_name}': {reason}\"\n        if self.existing_methods:\n            available = \", \".join(self.existing_methods)\n            message += f\". Available methods: {available}\"\n\n        super().__init__(message)\n</code></pre>"},{"location":"api_reference/exceptions/#quick_metric._exceptions.MethodNotFoundError","title":"<code>MethodNotFoundError</code>","text":"<p>               Bases: <code>MetricMethodError</code></p> <p>Exception raised when a requested method is not registered.</p> Source code in <code>quick_metric/_exceptions.py</code> <pre><code>class MethodNotFoundError(MetricMethodError):\n    \"\"\"Exception raised when a requested method is not registered.\"\"\"\n\n    def __init__(self, method_name: str, available_methods: list[str]):\n        self.method_name = method_name\n        self.available_methods = available_methods\n\n        methods_list = \", \".join(available_methods) if available_methods else \"None\"\n        message = (\n            f\"Metric method '{method_name}' is not registered. Available methods: {methods_list}\"\n        )\n\n        if available_methods:\n            # Suggest similar method names\n            similar = [m for m in available_methods if method_name.lower() in m.lower()]\n            if similar:\n                similar_list = \", \".join(similar)\n                message += f\". Did you mean one of: {similar_list}?\"\n\n        super().__init__(message)\n</code></pre>"},{"location":"api_reference/exceptions/#quick_metric._exceptions.MethodExecutionError","title":"<code>MethodExecutionError</code>","text":"<p>               Bases: <code>MetricMethodError</code></p> <p>Exception raised when a metric method fails during execution.</p> Source code in <code>quick_metric/_exceptions.py</code> <pre><code>class MethodExecutionError(MetricMethodError):\n    \"\"\"Exception raised when a metric method fails during execution.\"\"\"\n\n    def __init__(\n        self,\n        method_name: str,\n        original_error: Exception,\n        data_info: Optional[str] = None,\n    ):\n        self.method_name = method_name\n        self.original_error = original_error\n        self.data_info = data_info\n\n        message = f\"Metric method '{method_name}' failed during execution: {str(original_error)}\"\n        if data_info:\n            message += f\". Data info: {data_info}\"\n\n        super().__init__(message)\n</code></pre>"},{"location":"api_reference/exceptions/#quick_metric._exceptions.InvalidMethodSignatureError","title":"<code>InvalidMethodSignatureError</code>","text":"<p>               Bases: <code>MetricMethodError</code></p> <p>Exception raised when method has invalid signature for metric use.</p> Source code in <code>quick_metric/_exceptions.py</code> <pre><code>class InvalidMethodSignatureError(MetricMethodError):\n    \"\"\"Exception raised when method has invalid signature for metric use.\"\"\"\n\n    def __init__(self, method_name: str, signature_issue: str):\n        self.method_name = method_name\n        self.signature_issue = signature_issue\n\n        message = (\n            f\"Method '{method_name}' has invalid signature for metric use: \"\n            f\"{signature_issue}. Metric methods should accept at least one \"\n            f\"parameter (the data).\"\n        )\n\n        super().__init__(message)\n</code></pre>"},{"location":"api_reference/exceptions/#quick_metric._exceptions.MethodValidationError","title":"<code>MethodValidationError</code>","text":"<p>               Bases: <code>MetricMethodError</code></p> <p>Exception raised when method validation fails.</p> Source code in <code>quick_metric/_exceptions.py</code> <pre><code>class MethodValidationError(MetricMethodError):\n    \"\"\"Exception raised when method validation fails.\"\"\"\n\n    def __init__(self, method_name: str, validation_issue: str):\n        self.method_name = method_name\n        self.validation_issue = validation_issue\n\n        message = f\"Method '{method_name}' validation failed: {validation_issue}\"\n        super().__init__(message)\n</code></pre>"},{"location":"api_reference/exceptions/#quick_metric._exceptions.DuplicateMethodWarning","title":"<code>DuplicateMethodWarning</code>","text":"<p>               Bases: <code>LoggedException</code></p> <p>Warning raised when a method is registered multiple times.</p> Source code in <code>quick_metric/_exceptions.py</code> <pre><code>class DuplicateMethodWarning(LoggedException):\n    \"\"\"Warning raised when a method is registered multiple times.\"\"\"\n\n    def __init__(self, method_name: str, source_info: Optional[str] = None):\n        self.method_name = method_name\n        self.source_info = source_info\n\n        message = (\n            f\"Method '{method_name}' is being re-registered, overwriting previous registration\"\n        )\n        if source_info:\n            message += f\" from {source_info}\"\n\n        super().__init__(message)\n</code></pre>"},{"location":"api_reference/exceptions/#quick_metric._exceptions.RegistryLockError","title":"<code>RegistryLockError</code>","text":"<p>               Bases: <code>MetricMethodError</code></p> <p>Exception raised when registry operations fail due to threading.</p> Source code in <code>quick_metric/_exceptions.py</code> <pre><code>class RegistryLockError(MetricMethodError):\n    \"\"\"Exception raised when registry operations fail due to threading.\"\"\"\n\n    def __init__(self, operation: str, reason: str):\n        self.operation = operation\n        self.reason = reason\n\n        message = f\"Registry {operation} failed due to threading issue: {reason}\"\n        super().__init__(message)\n</code></pre>"},{"location":"api_reference/exceptions/#quick_metric._exceptions.EmptyRegistryError","title":"<code>EmptyRegistryError</code>","text":"<p>               Bases: <code>MetricMethodError</code></p> <p>Exception raised when attempting operations on empty registry.</p> Source code in <code>quick_metric/_exceptions.py</code> <pre><code>class EmptyRegistryError(MetricMethodError):\n    \"\"\"Exception raised when attempting operations on empty registry.\"\"\"\n\n    def __init__(self, operation: str):\n        self.operation = operation\n\n        message = (\n            f\"Cannot perform {operation} on empty method registry. \"\n            f\"Register some methods first using @metric_method decorator.\"\n        )\n\n        super().__init__(message)\n</code></pre>"},{"location":"api_reference/exceptions/#quick_metric._exceptions.MetricSpecificationError","title":"<code>MetricSpecificationError</code>","text":"<p>               Bases: <code>ValueError</code>, <code>LoggedException</code></p> <p>Exception raised when metric specification is invalid.</p> Source code in <code>quick_metric/_exceptions.py</code> <pre><code>class MetricSpecificationError(ValueError, LoggedException):\n    \"\"\"Exception raised when metric specification is invalid.\"\"\"\n\n    def __init__(self, specification_issue: str, method_spec=None):\n        self.specification_issue = specification_issue\n        self.method_spec = method_spec\n\n        message = f\"Invalid metric specification: {specification_issue}\"\n        if method_spec:\n            message += f\". Method specification: {method_spec}\"\n\n        super().__init__(message)\n</code></pre>"},{"location":"api_reference/exceptions/#quick_metric._exceptions.MetricsMethodNotFoundError","title":"<code>MetricsMethodNotFoundError</code>","text":"<p>               Bases: <code>MetricMethodError</code></p> <p>Exception raised when a specified method is not found in metrics methods.</p> Source code in <code>quick_metric/_exceptions.py</code> <pre><code>class MetricsMethodNotFoundError(MetricMethodError):\n    \"\"\"Exception raised when a specified method is not found in metrics methods.\"\"\"\n\n    def __init__(self, method_name: str, available_methods: list[str]):\n        self.method_name = method_name\n        self.available_methods = available_methods\n\n        methods_list = \", \".join(available_methods) if available_methods else \"None\"\n        message = (\n            f\"Metric method '{method_name}' is not registered. Available methods: {methods_list}\"\n        )\n\n        if available_methods:\n            # Use difflib to suggest similar method names\n            close_matches = difflib.get_close_matches(\n                method_name, available_methods, n=3, cutoff=0.4\n            )\n            if close_matches:\n                suggestions = \", \".join(close_matches)\n                message += f\". Did you mean one of: {suggestions}?\"\n\n        super().__init__(message)\n</code></pre>"},{"location":"api_reference/filter/","title":"Filter","text":"<p>Data filtering functionality for Quick Metric.</p> <p>This module provides comprehensive data filtering capabilities for pandas DataFrames using dictionary-based filter specifications. It supports complex logical operations including AND, OR, NOT conditions, as well as various comparison operators and membership tests.</p> <p>The filtering system is designed to work with nested filter conditions specified in YAML configurations, allowing for sophisticated data subset selection before applying metric methods.</p> <p>Functions:</p> Name Description <code>evaluate_condition : Evaluate a single filter condition</code> <code>recursive_filter : Handle complex nested filter conditions</code> <code>apply_filter : Main entry point for applying filters to DataFrames</code> Supported Operators <ul> <li>Equality: <code>{'column': 'value'}</code></li> <li>Membership: <code>{'column': ['value1', 'value2']}</code></li> <li>Comparisons: <code>{'column': {'greater than': 10}}</code></li> <li>Logical: <code>{'and': {...}}</code>, <code>{'or': {...}}</code>, <code>{'not': {...}}</code></li> <li>Set operations: <code>{'column': {'in': [...]}}</code></li> </ul> <p>Examples:</p> <p>Simple equality filter:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from quick_metric._filter import apply_filter\n&gt;&gt;&gt;\n&gt;&gt;&gt; data = pd.DataFrame({\n...     'category': ['A', 'B', 'A', 'C'],\n...     'value': [10, 20, 30, 40]\n... })\n&gt;&gt;&gt; filter_spec = {'category': 'A'}\n&gt;&gt;&gt; filtered_data = apply_filter(data, filter_spec)\n&gt;&gt;&gt; print(len(filtered_data))\n2\n</code></pre> <p>Complex nested filter:</p> <pre><code>&gt;&gt;&gt; filter_spec = {\n...     'and': {\n...         'category': ['A', 'B'],\n...         'value': {'greater than': 15}\n...     }\n... }\n&gt;&gt;&gt; filtered_data = apply_filter(data, filter_spec)\n&gt;&gt;&gt; print(len(filtered_data))\n2\n</code></pre> <p>Negation filter:</p> <pre><code>&gt;&gt;&gt; filter_spec = {\n...     'not': {'category': 'C'}\n... }\n&gt;&gt;&gt; filtered_data = apply_filter(data, filter_spec)\n&gt;&gt;&gt; print(len(filtered_data))\n3\n</code></pre> See Also <p>interpret_instructions : Module that uses filtering before applying methods apply_methods : Module that processes filtered data</p>"},{"location":"api_reference/filter/#quick_metric._filter.evaluate_condition","title":"<code>evaluate_condition(data_df, column, value)</code>","text":"<p>Evaluate a condition based on the provided column and value.</p> <p>Parameters:</p> Name Type Description Default <code>data_df</code> <code>DataFrame</code> <p>The DataFrame to be filtered.</p> required <code>column</code> <code>str</code> <p>The column name to be filtered.</p> required <code>value</code> <code>Union[dict, Any]</code> <p>The value to be compared against the column.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Boolean filter mask indicating whether the condition is met for each row in the DataFrame.</p> Notes <p>This function supports the following comparison operators: - less than - less than equal - greater than - greater than equal - is - not - in - not in</p> Source code in <code>quick_metric/_filter.py</code> <pre><code>def evaluate_condition(  # noqa: PLR0911\n    data_df: pd.DataFrame, column: str, value: Union[dict, Any]\n) -&gt; pd.Series:\n    \"\"\"\n    Evaluate a condition based on the provided column and value.\n\n    Parameters\n    ----------\n    data_df : pd.DataFrame\n        The DataFrame to be filtered.\n    column : str\n        The column name to be filtered.\n    value : Union[dict, Any]\n        The value to be compared against the column.\n\n    Returns\n    -------\n    pd.Series\n        Boolean filter mask indicating whether the condition is met\n        for each row in the DataFrame.\n\n    Notes\n    -----\n    This function supports the following comparison operators:\n    - less than\n    - less than equal\n    - greater than\n    - greater than equal\n    - is\n    - not\n    - in\n    - not in\n    \"\"\"\n    if isinstance(value, dict):\n        if \"not\" in value:\n            return ~evaluate_condition(data_df, column, value[\"not\"])\n        if \"less than\" in value:\n            return data_df[column] &lt; value[\"less than\"]\n        if \"less than equal\" in value:\n            return data_df[column] &lt;= value[\"less than equal\"]\n        if \"greater than\" in value:\n            return data_df[column] &gt; value[\"greater than\"]\n        if \"greater than equal\" in value:\n            return data_df[column] &gt;= value[\"greater than equal\"]\n        if \"is\" in value:\n            return data_df[column] == value[\"is\"]\n        if \"in\" in value:\n            return data_df[column].isin(value[\"in\"])\n        if \"not in\" in value:\n            return ~data_df[column].isin(value[\"not in\"])\n    if column in data_df.columns:\n        if isinstance(value, list):\n            return data_df[column].isin(value)\n        return data_df[column] == value\n    # Return a boolean mask with all False values\n    return pd.Series(index=data_df.index, data=False, dtype=bool)\n</code></pre>"},{"location":"api_reference/filter/#quick_metric._filter.recursive_filter","title":"<code>recursive_filter(data_df, filters)</code>","text":"<p>Recursively applies filters to a DataFrame and returns a boolean mask.</p> <p>Parameters:</p> Name Type Description Default <code>data_df</code> <code>DataFrame</code> <p>The DataFrame to filter.</p> required <code>filters</code> <code>dict</code> <p>A dictionary specifying the filters to apply. The dictionary can contain the keys \"and\", \"or\", and \"not\" to combine conditions recursively.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>A boolean filter mask indicating which rows of the DataFrame match the filters.</p> Notes <p>The filters dictionary can have the following structure: - {\"and\": {condition1, condition2, ...}}: All conditions must be met. - {\"or\": {condition1, condition2, ...}}: At least one condition must   be met. - {\"not\": {condition}}: The condition must not be met. - {column: value}: A single condition to apply to a column.</p> <p>Each condition is a key-value pair where the key is the column name and the value is the condition to apply to that column.</p> Source code in <code>quick_metric/_filter.py</code> <pre><code>def recursive_filter(data_df: pd.DataFrame, filters: dict) -&gt; pd.Series:\n    \"\"\"\n    Recursively applies filters to a DataFrame and returns a boolean mask.\n\n    Parameters\n    ----------\n    data_df : pd.DataFrame\n        The DataFrame to filter.\n    filters : dict\n        A dictionary specifying the filters to apply. The dictionary can\n        contain the keys \"and\", \"or\", and \"not\" to combine conditions\n        recursively.\n\n    Returns\n    -------\n    pd.Series\n        A boolean filter mask indicating which rows of the DataFrame match\n        the filters.\n\n    Notes\n    -----\n    The filters dictionary can have the following structure:\n    - {\"and\": {condition1, condition2, ...}}: All conditions must be met.\n    - {\"or\": {condition1, condition2, ...}}: At least one condition must\n      be met.\n    - {\"not\": {condition}}: The condition must not be met.\n    - {column: value}: A single condition to apply to a column.\n\n    Each condition is a key-value pair where the key is the column name\n    and the value is the condition to apply to that column.\n    \"\"\"\n    # Handle empty filters - return all rows as True\n    if not filters:\n        return pd.Series(index=data_df.index, data=True, dtype=bool)\n\n    if \"and\" in filters:\n        mask = pd.Series(index=data_df.index, data=True, dtype=bool)\n        for key, value in filters[\"and\"].items():\n            if key in [\"and\", \"or\", \"not\"]:\n                mask &amp;= recursive_filter(data_df, {key: value})\n            else:\n                condition_result = evaluate_condition(data_df, key, value)\n                if condition_result is not None:\n                    mask &amp;= condition_result\n        return mask\n    if \"or\" in filters:\n        mask = pd.Series(index=data_df.index, data=False, dtype=bool)\n        or_conditions = filters[\"or\"]\n\n        # Handle both list and dict format for or conditions\n        if isinstance(or_conditions, list):\n            for condition in or_conditions:\n                condition_result = recursive_filter(data_df, condition)\n                mask |= condition_result\n        else:\n            # Dictionary format (original logic)\n            for key, value in or_conditions.items():\n                if key in [\"and\", \"or\", \"not\"]:\n                    condition_result = recursive_filter(data_df, {key: value})\n                    mask |= condition_result\n                else:\n                    condition_result = evaluate_condition(data_df, key, value)\n                    if condition_result is not None:\n                        mask |= condition_result\n        return mask\n    if \"not\" in filters:\n        return ~recursive_filter(data_df, filters[\"not\"])\n    return evaluate_condition(data_df, list(filters.keys())[0], list(filters.values())[0])\n</code></pre>"},{"location":"api_reference/filter/#quick_metric._filter.apply_filter","title":"<code>apply_filter(data_df, filters)</code>","text":"<p>Apply filters to the DataFrame based on the provided dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data_df</code> <code>DataFrame</code> <p>The DataFrame to be filtered.</p> required <code>filters</code> <code>dict</code> <p>Dictionary containing the filter conditions.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Filtered DataFrame.</p> Source code in <code>quick_metric/_filter.py</code> <pre><code>def apply_filter(data_df: pd.DataFrame, filters: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Apply filters to the DataFrame based on the provided dictionary.\n\n    Parameters\n    ----------\n    data_df : pd.DataFrame\n        The DataFrame to be filtered.\n    filters : dict\n        Dictionary containing the filter conditions.\n\n    Returns\n    -------\n    pd.DataFrame\n        Filtered DataFrame.\n    \"\"\"\n    logger.trace(f\"Applying filters to DataFrame with {len(data_df)} rows\")\n\n    if not filters:\n        logger.trace(\"No filters specified, returning original DataFrame\")\n        return data_df\n\n    mask = recursive_filter(data_df, filters)\n    filtered_df = data_df.loc[mask]  # type: ignore[return-value]\n\n    logger.trace(f\"Filter applied: {len(filtered_df)} rows remaining\")\n    return filtered_df\n</code></pre>"},{"location":"api_reference/method_definitions/","title":"Method Definitions","text":"<p>Method registration and decorator functionality for Quick Metric.</p> <p>This module provides the core decorator for registering custom metric methods that can be used with the quick_metric framework. Methods decorated with @metric_method are automatically registered in the global registry and become available for use in YAML configurations.</p>"},{"location":"api_reference/method_definitions/#quick_metric._method_definitions.MetricRegistry","title":"<code>MetricRegistry</code>","text":"<p>Thread-safe registry for user-defined metric methods.</p> <p>Provides a centralized, thread-safe way to register and access metric methods decorated with @metric_method.</p> Source code in <code>quick_metric/_method_definitions.py</code> <pre><code>class MetricRegistry:\n    \"\"\"\n    Thread-safe registry for user-defined metric methods.\n\n    Provides a centralized, thread-safe way to register and access metric\n    methods decorated with @metric_method.\n    \"\"\"\n\n    def __init__(self):\n        self._methods: dict[str, Callable] = {}\n        self._lock = threading.RLock()\n\n    def register(self, func: Callable) -&gt; Callable:\n        \"\"\"\n        Register a user function as a metric method.\n\n        Parameters\n        ----------\n        func : Callable\n            User function to register. Must accept at least one parameter.\n\n        Returns\n        -------\n        Callable\n            The original function, unchanged.\n\n        Raises\n        ------\n        InvalidMethodSignatureError\n            If function has invalid signature.\n        \"\"\"\n        logger.trace(f\"Registering metric method: {func.__name__}\")\n\n        # Validate function signature\n        sig = inspect.signature(func)\n        if not sig.parameters:\n            raise InvalidMethodSignatureError(\n                func.__name__,\n                \"Function must accept at least one parameter (the data)\",\n            )\n\n        # Register the method\n        with self._lock:\n            if func.__name__ in self._methods:\n                logger.warning(f\"Method '{func.__name__}' already registered, overwriting\")\n            self._methods[func.__name__] = func\n\n        logger.success(f\"Metric method '{func.__name__}' registered\")\n        return func\n\n    def get_method(self, name: str) -&gt; Callable:\n        \"\"\"\n        Get a registered method by name.\n\n        Parameters\n        ----------\n        name : str\n            Name of the method.\n\n        Returns\n        -------\n        Callable\n            The registered method.\n\n        Raises\n        ------\n        MethodNotFoundError\n            If method not registered.\n        \"\"\"\n        with self._lock:\n            if name not in self._methods:\n                available = list(self._methods.keys())\n                logger.error(f\"Method '{name}' not found. Available: {available}\")\n                raise MethodNotFoundError(name, available)\n            return self._methods[name]\n\n    def get_methods(self) -&gt; dict[str, Callable]:\n        \"\"\"\n        Get copy of all registered methods.\n\n        Returns\n        -------\n        Dict[str, Callable]\n            Copy of methods dictionary.\n\n        Raises\n        ------\n        RegistryLockError\n            If threading lock fails.\n        \"\"\"\n        try:\n            with self._lock:\n                return self._methods.copy()\n        except Exception as e:\n            raise RegistryLockError(\"get_methods\", str(e)) from e\n\n    def list_method_names(self) -&gt; list[str]:\n        \"\"\"\n        Get list of registered method names.\n\n        Returns\n        -------\n        list[str]\n            List of method names.\n\n        Raises\n        ------\n        EmptyRegistryError\n            If no methods registered.\n        RegistryLockError\n            If threading lock fails.\n        \"\"\"\n        try:\n            with self._lock:\n                if not self._methods:\n                    raise EmptyRegistryError(\"list_method_names\")\n                return list(self._methods.keys())\n        except EmptyRegistryError:\n            raise\n        except Exception as e:\n            raise RegistryLockError(\"list_method_names\", str(e)) from e\n\n    def clear(self) -&gt; None:\n        \"\"\"\n        Clear all registered methods (for testing).\n\n        Raises\n        ------\n        RegistryLockError\n            If threading lock fails.\n        \"\"\"\n        logger.debug(\"Clearing all registered metric methods\")\n        try:\n            with self._lock:\n                self._methods.clear()\n        except Exception as e:\n            raise RegistryLockError(\"clear_methods\", str(e)) from e\n        logger.success(\"All metric methods cleared\")\n</code></pre>"},{"location":"api_reference/method_definitions/#quick_metric._method_definitions.MetricRegistry.register","title":"<code>register(func)</code>","text":"<p>Register a user function as a metric method.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>User function to register. Must accept at least one parameter.</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>The original function, unchanged.</p> <p>Raises:</p> Type Description <code>InvalidMethodSignatureError</code> <p>If function has invalid signature.</p> Source code in <code>quick_metric/_method_definitions.py</code> <pre><code>def register(self, func: Callable) -&gt; Callable:\n    \"\"\"\n    Register a user function as a metric method.\n\n    Parameters\n    ----------\n    func : Callable\n        User function to register. Must accept at least one parameter.\n\n    Returns\n    -------\n    Callable\n        The original function, unchanged.\n\n    Raises\n    ------\n    InvalidMethodSignatureError\n        If function has invalid signature.\n    \"\"\"\n    logger.trace(f\"Registering metric method: {func.__name__}\")\n\n    # Validate function signature\n    sig = inspect.signature(func)\n    if not sig.parameters:\n        raise InvalidMethodSignatureError(\n            func.__name__,\n            \"Function must accept at least one parameter (the data)\",\n        )\n\n    # Register the method\n    with self._lock:\n        if func.__name__ in self._methods:\n            logger.warning(f\"Method '{func.__name__}' already registered, overwriting\")\n        self._methods[func.__name__] = func\n\n    logger.success(f\"Metric method '{func.__name__}' registered\")\n    return func\n</code></pre>"},{"location":"api_reference/method_definitions/#quick_metric._method_definitions.MetricRegistry.get_method","title":"<code>get_method(name)</code>","text":"<p>Get a registered method by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the method.</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>The registered method.</p> <p>Raises:</p> Type Description <code>MethodNotFoundError</code> <p>If method not registered.</p> Source code in <code>quick_metric/_method_definitions.py</code> <pre><code>def get_method(self, name: str) -&gt; Callable:\n    \"\"\"\n    Get a registered method by name.\n\n    Parameters\n    ----------\n    name : str\n        Name of the method.\n\n    Returns\n    -------\n    Callable\n        The registered method.\n\n    Raises\n    ------\n    MethodNotFoundError\n        If method not registered.\n    \"\"\"\n    with self._lock:\n        if name not in self._methods:\n            available = list(self._methods.keys())\n            logger.error(f\"Method '{name}' not found. Available: {available}\")\n            raise MethodNotFoundError(name, available)\n        return self._methods[name]\n</code></pre>"},{"location":"api_reference/method_definitions/#quick_metric._method_definitions.MetricRegistry.get_methods","title":"<code>get_methods()</code>","text":"<p>Get copy of all registered methods.</p> <p>Returns:</p> Type Description <code>Dict[str, Callable]</code> <p>Copy of methods dictionary.</p> <p>Raises:</p> Type Description <code>RegistryLockError</code> <p>If threading lock fails.</p> Source code in <code>quick_metric/_method_definitions.py</code> <pre><code>def get_methods(self) -&gt; dict[str, Callable]:\n    \"\"\"\n    Get copy of all registered methods.\n\n    Returns\n    -------\n    Dict[str, Callable]\n        Copy of methods dictionary.\n\n    Raises\n    ------\n    RegistryLockError\n        If threading lock fails.\n    \"\"\"\n    try:\n        with self._lock:\n            return self._methods.copy()\n    except Exception as e:\n        raise RegistryLockError(\"get_methods\", str(e)) from e\n</code></pre>"},{"location":"api_reference/method_definitions/#quick_metric._method_definitions.MetricRegistry.list_method_names","title":"<code>list_method_names()</code>","text":"<p>Get list of registered method names.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of method names.</p> <p>Raises:</p> Type Description <code>EmptyRegistryError</code> <p>If no methods registered.</p> <code>RegistryLockError</code> <p>If threading lock fails.</p> Source code in <code>quick_metric/_method_definitions.py</code> <pre><code>def list_method_names(self) -&gt; list[str]:\n    \"\"\"\n    Get list of registered method names.\n\n    Returns\n    -------\n    list[str]\n        List of method names.\n\n    Raises\n    ------\n    EmptyRegistryError\n        If no methods registered.\n    RegistryLockError\n        If threading lock fails.\n    \"\"\"\n    try:\n        with self._lock:\n            if not self._methods:\n                raise EmptyRegistryError(\"list_method_names\")\n            return list(self._methods.keys())\n    except EmptyRegistryError:\n        raise\n    except Exception as e:\n        raise RegistryLockError(\"list_method_names\", str(e)) from e\n</code></pre>"},{"location":"api_reference/method_definitions/#quick_metric._method_definitions.MetricRegistry.clear","title":"<code>clear()</code>","text":"<p>Clear all registered methods (for testing).</p> <p>Raises:</p> Type Description <code>RegistryLockError</code> <p>If threading lock fails.</p> Source code in <code>quick_metric/_method_definitions.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"\n    Clear all registered methods (for testing).\n\n    Raises\n    ------\n    RegistryLockError\n        If threading lock fails.\n    \"\"\"\n    logger.debug(\"Clearing all registered metric methods\")\n    try:\n        with self._lock:\n            self._methods.clear()\n    except Exception as e:\n        raise RegistryLockError(\"clear_methods\", str(e)) from e\n    logger.success(\"All metric methods cleared\")\n</code></pre>"},{"location":"api_reference/method_definitions/#quick_metric._method_definitions.metric_method","title":"<code>metric_method(func_or_name=None)</code>","text":"<p>Decorator to register a user function as a metric method, or query registered methods.</p> <p>Can be used in three ways: 1. As a decorator: @metric_method 2. To get all methods: metric_method() 3. To get a specific method: metric_method('method_name')</p> <p>Parameters:</p> Name Type Description Default <code>func_or_name</code> <code>Callable or str</code> <p>User function to register when used as decorator, or method name to retrieve.</p> <code>None</code> <p>Returns:</p> Type Description <code>Callable or dict</code> <p>When used as decorator: returns the original function unchanged. When called without args: returns dict of all registered methods. When called with method name: returns the specific method.</p> <p>Examples:</p> <p>As a decorator:</p> <pre><code>&gt;&gt;&gt; @metric_method\n... def my_custom_metric(data):\n...     return len(data)\n</code></pre> <p>To get all methods:</p> <pre><code>&gt;&gt;&gt; all_methods = metric_method()\n&gt;&gt;&gt; print(list(all_methods.keys()))\n</code></pre> <p>To get a specific method:</p> <pre><code>&gt;&gt;&gt; my_method = metric_method('my_custom_metric')\n</code></pre> Source code in <code>quick_metric/_method_definitions.py</code> <pre><code>def metric_method(func_or_name=None):\n    \"\"\"\n    Decorator to register a user function as a metric method, or query registered methods.\n\n    Can be used in three ways:\n    1. As a decorator: @metric_method\n    2. To get all methods: metric_method()\n    3. To get a specific method: metric_method('method_name')\n\n    Parameters\n    ----------\n    func_or_name : Callable or str, optional\n        User function to register when used as decorator, or method name to retrieve.\n\n    Returns\n    -------\n    Callable or dict\n        When used as decorator: returns the original function unchanged.\n        When called without args: returns dict of all registered methods.\n        When called with method name: returns the specific method.\n\n    Examples\n    --------\n    As a decorator:\n    &gt;&gt;&gt; @metric_method\n    ... def my_custom_metric(data):\n    ...     return len(data)\n\n    To get all methods:\n    &gt;&gt;&gt; all_methods = metric_method()\n    &gt;&gt;&gt; print(list(all_methods.keys()))\n\n    To get a specific method:\n    &gt;&gt;&gt; my_method = metric_method('my_custom_metric')\n    \"\"\"\n    # Case 1: Called without arguments - return all methods\n    if func_or_name is None:\n        return _registry.get_methods()\n\n    # Case 2: Called with a string - return specific method\n    if isinstance(func_or_name, str):\n        return _registry.get_method(func_or_name)\n\n    # Case 3: Called with a function (decorator usage)\n    if callable(func_or_name):\n        return _registry.register(func_or_name)\n\n    raise ValueError(f\"Invalid argument type: {type(func_or_name)}\")\n</code></pre>"},{"location":"api_reference/method_definitions/#quick_metric._method_definitions.get_method","title":"<code>get_method(name)</code>","text":"<p>Get a registered method by name.</p> Source code in <code>quick_metric/_method_definitions.py</code> <pre><code>def get_method(name: str) -&gt; Callable:\n    \"\"\"Get a registered method by name.\"\"\"\n    return _registry.get_method(name)\n</code></pre>"},{"location":"api_reference/method_definitions/#quick_metric._method_definitions.get_registered_methods","title":"<code>get_registered_methods()</code>","text":"<p>Get all registered methods.</p> Source code in <code>quick_metric/_method_definitions.py</code> <pre><code>def get_registered_methods() -&gt; dict[str, Callable]:\n    \"\"\"Get all registered methods.\"\"\"\n    return _registry.get_methods()\n</code></pre>"},{"location":"api_reference/method_definitions/#quick_metric._method_definitions.list_method_names","title":"<code>list_method_names()</code>","text":"<p>Get list of registered method names.</p> Source code in <code>quick_metric/_method_definitions.py</code> <pre><code>def list_method_names() -&gt; list[str]:\n    \"\"\"Get list of registered method names.\"\"\"\n    return _registry.list_method_names()\n</code></pre>"},{"location":"api_reference/method_definitions/#quick_metric._method_definitions.clear_methods","title":"<code>clear_methods()</code>","text":"<p>Clear all registered methods (for testing).</p> Source code in <code>quick_metric/_method_definitions.py</code> <pre><code>def clear_methods() -&gt; None:\n    \"\"\"Clear all registered methods (for testing).\"\"\"\n    _registry.clear()\n</code></pre>"},{"location":"api_reference/output_formats/","title":"Output Formats","text":"<p>Output format handling for Quick Metric results.</p> <p>This module provides functionality to convert metric results between different output formats while keeping complex data types (like DataFrames) intact.</p> <p>The module follows the KISS principle by not over-flattening complex results and maintaining the integrity of pandas DataFrames and Series objects.</p>"},{"location":"api_reference/output_formats/#quick_metric._output_formats.OutputFormat","title":"<code>OutputFormat</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Supported output formats for generated metrics.</p> Source code in <code>quick_metric/_output_formats.py</code> <pre><code>class OutputFormat(Enum):\n    \"\"\"Supported output formats for generated metrics.\"\"\"\n\n    NESTED = \"nested\"  # Current dict of dicts: {'metric': {'method': result}}\n    DATAFRAME = \"dataframe\"  # Long format DataFrame with metric/method/value columns\n    FLAT_DATAFRAME = \"flat_dataframe\"  # Completely flat DataFrame (expands nested results)\n    RECORDS = \"records\"  # List of dicts: [{'metric': '...', 'method': '...', 'value': ...}]\n</code></pre>"},{"location":"api_reference/output_formats/#quick_metric._output_formats.convert_to_records","title":"<code>convert_to_records(results)</code>","text":"<p>Convert nested results to records format.</p> <p>Keeps complex data types (DataFrames, Series) intact rather than flattening.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>dict</code> <p>Nested results in format {'metric': {'method': result}}</p> required <p>Returns:</p> Type Description <code>List[dict]</code> <p>Records with structure [{'metric': str, 'method': str, 'value': Any}]</p> Source code in <code>quick_metric/_output_formats.py</code> <pre><code>def convert_to_records(results: dict) -&gt; list[dict]:\n    \"\"\"\n    Convert nested results to records format.\n\n    Keeps complex data types (DataFrames, Series) intact rather than flattening.\n\n    Parameters\n    ----------\n    results : dict\n        Nested results in format {'metric': {'method': result}}\n\n    Returns\n    -------\n    List[dict]\n        Records with structure [{'metric': str, 'method': str, 'value': Any}]\n    \"\"\"\n    records = []\n\n    for metric_name, metric_results in results.items():\n        for method_name, result in metric_results.items():\n            record = {\n                \"metric\": metric_name,\n                \"method\": method_name,\n                \"value\": result,\n                \"value_type\": type(result).__name__,\n            }\n            records.append(record)\n\n    return records\n</code></pre>"},{"location":"api_reference/output_formats/#quick_metric._output_formats.convert_to_dataframe","title":"<code>convert_to_dataframe(results)</code>","text":"<p>Convert nested results to DataFrame format.</p> <p>Creates a long-format DataFrame while preserving complex data types. For DataFrames and Series, stores the object reference rather than flattening.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>dict</code> <p>Nested results in format {'metric': {'method': result}}</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Long format DataFrame with columns [metric, method, value, value_type]</p> Source code in <code>quick_metric/_output_formats.py</code> <pre><code>def convert_to_dataframe(results: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Convert nested results to DataFrame format.\n\n    Creates a long-format DataFrame while preserving complex data types.\n    For DataFrames and Series, stores the object reference rather than flattening.\n\n    Parameters\n    ----------\n    results : dict\n        Nested results in format {'metric': {'method': result}}\n\n    Returns\n    -------\n    pd.DataFrame\n        Long format DataFrame with columns [metric, method, value, value_type]\n    \"\"\"\n    records = convert_to_records(results)\n    if not records:\n        # Create empty DataFrame with expected columns\n        return pd.DataFrame(columns=[\"metric\", \"method\", \"value\", \"value_type\"])\n    return pd.DataFrame(records)\n</code></pre>"},{"location":"api_reference/output_formats/#quick_metric._output_formats.convert_to_flat_dataframe","title":"<code>convert_to_flat_dataframe(results)</code>","text":"<p>Convert nested results to a completely flat DataFrame format.</p> <p>This format creates one row per data point with clean, simple columns: - metric: metric name - method: method name - group_by: grouping variable(s) - single values or tuples for multiple groups - statistic: the statistic/column being measured - metric_value: the actual metric value</p> <p>Benefits of using tuples in group_by: - Preserves grouping structure instead of concatenating strings - Allows filtering by individual grouping levels:   result[result['group_by'].apply(lambda x: x[0] == 'East')] - Maintains data types instead of converting everything to strings</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>dict</code> <p>Nested results dictionary</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Flat DataFrame with one row per data point</p> Source code in <code>quick_metric/_output_formats.py</code> <pre><code>def convert_to_flat_dataframe(results: dict) -&gt; pd.DataFrame:\n    \"\"\"\n    Convert nested results to a completely flat DataFrame format.\n\n    This format creates one row per data point with clean, simple columns:\n    - metric: metric name\n    - method: method name\n    - group_by: grouping variable(s) - single values or tuples for multiple groups\n    - statistic: the statistic/column being measured\n    - metric_value: the actual metric value\n\n    Benefits of using tuples in group_by:\n    - Preserves grouping structure instead of concatenating strings\n    - Allows filtering by individual grouping levels:\n      result[result['group_by'].apply(lambda x: x[0] == 'East')]\n    - Maintains data types instead of converting everything to strings\n\n    Parameters\n    ----------\n    results : dict\n        Nested results dictionary\n\n    Returns\n    -------\n    pd.DataFrame\n        Flat DataFrame with one row per data point\n    \"\"\"\n    dataframes_to_concat = []\n\n    for metric_name, metric_results in results.items():\n        for method_name, result in metric_results.items():\n            if isinstance(result, pd.DataFrame):\n                # Handle DataFrame results\n                df_reset = result.reset_index()\n\n                # Store original column structure for statistic tuples\n                original_columns = result.columns\n                col_mapping = None\n\n                # Check if the original index was just a default RangeIndex\n                has_meaningful_index = (\n                    not isinstance(result.index, pd.RangeIndex) or result.index.name is not None\n                )\n\n                # For MultiIndex columns, create mapping before flattening\n                if isinstance(original_columns, pd.MultiIndex):\n                    # Create a mapping from flattened names to original tuples\n                    col_mapping = {}\n                    flattened_cols = []\n                    for col in df_reset.columns:\n                        if isinstance(col, tuple):\n                            flattened = \"_\".join(str(i) for i in col if str(i) != \"\").strip(\"_\")\n                            col_mapping[flattened] = col\n                            flattened_cols.append(flattened)\n                        else:\n                            col_mapping[col] = col\n                            flattened_cols.append(col)\n\n                    # Temporarily flatten for processing\n                    df_reset.columns = flattened_cols\n\n                # Find index columns (those added by reset_index)\n                if isinstance(original_columns, pd.MultiIndex):\n                    original_col_names = {\n                        \"_\".join(str(i) for i in col if str(i) != \"\").strip(\"_\")\n                        for col in original_columns\n                    }\n                else:\n                    original_col_names = set(original_columns)\n\n                # Find which columns are index columns\n                index_cols = [col for col in df_reset.columns if col not in original_col_names]\n\n                # Handle the case where reset_index creates default 'index' column\n                if not index_cols and \"index\" in df_reset.columns:\n                    index_cols = [\"index\"]\n\n                # Create group_by column using tuples to preserve structure\n                if index_cols and has_meaningful_index:\n                    if len(index_cols) == 1:\n                        df_reset[\"group_by\"] = df_reset[index_cols[0]]\n                    else:\n                        # Use tuples instead of concatenating strings\n                        df_reset[\"group_by\"] = df_reset[index_cols].apply(\n                            lambda x: tuple(x), axis=1\n                        )\n                else:\n                    # No meaningful grouping - use None instead of arbitrary values\n                    df_reset[\"group_by\"] = None\n\n                # Melt the DataFrame\n                value_cols = [\n                    col for col in df_reset.columns if col not in index_cols + [\"group_by\"]\n                ]\n\n                df_melted = df_reset.melt(\n                    id_vars=[\"group_by\"],\n                    value_vars=value_cols,\n                    var_name=\"statistic\",\n                    value_name=\"metric_value\",\n                )\n\n                # Convert statistic column to tuples if it came from MultiIndex\n                if col_mapping is not None:\n                    df_melted[\"statistic\"] = df_melted[\"statistic\"].map(\n                        lambda x, mapping=col_mapping: mapping.get(x, x)\n                    )\n\n                # Add metadata\n                df_melted[\"metric\"] = metric_name\n                df_melted[\"method\"] = method_name\n\n                # Reorder columns\n                df_melted = df_melted[[\"metric\", \"method\", \"group_by\", \"statistic\", \"metric_value\"]]\n                dataframes_to_concat.append(df_melted)\n\n            elif isinstance(result, pd.Series):\n                # Handle Series results\n                series_df = result.reset_index()\n\n                # Handle MultiIndex series - use tuples for group_by\n                if len(series_df.columns) &gt; 2:\n                    # Multiple index columns - use tuples instead of concatenation\n                    index_cols = series_df.columns[:-1].tolist()  # All but the last (value) column\n                    series_df[\"group_by\"] = series_df[index_cols].apply(lambda x: tuple(x), axis=1)\n                    # Keep only group_by and the value column\n                    value_col = series_df.columns[-1]\n                    series_df = series_df[[\"group_by\", value_col]]\n                    series_df.columns = [\"group_by\", \"metric_value\"]\n                else:\n                    # Single index column\n                    series_df.columns = [\"group_by\", \"metric_value\"]\n\n                series_df[\"metric\"] = metric_name\n                series_df[\"method\"] = method_name\n                series_df[\"statistic\"] = result.name or \"value\"\n\n                # No need to convert group_by to string - keep tuples or original types\n\n                # Reorder columns\n                series_df = series_df[[\"metric\", \"method\", \"group_by\", \"statistic\", \"metric_value\"]]\n                dataframes_to_concat.append(series_df)\n\n            else:\n                # Handle scalar values\n                scalar_df = pd.DataFrame(\n                    {\n                        \"metric\": [metric_name],\n                        \"method\": [method_name],\n                        \"group_by\": [None],  # Use None for no grouping instead of 'all'\n                        \"statistic\": [\"value\"],\n                        \"metric_value\": [result],\n                    }\n                )\n                dataframes_to_concat.append(scalar_df)\n\n    if not dataframes_to_concat:\n        return pd.DataFrame(columns=[\"metric\", \"method\", \"group_by\", \"statistic\", \"metric_value\"])\n\n    # Efficiently concatenate all DataFrames at once\n    return pd.concat(dataframes_to_concat, ignore_index=True)\n</code></pre>"},{"location":"api_reference/output_formats/#quick_metric._output_formats.convert_to_format","title":"<code>convert_to_format(results, output_format)</code>","text":"<p>Convert nested results to the specified output format.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>dict</code> <p>Nested results from interpret_metric_instructions</p> required <code>output_format</code> <code>OutputFormat</code> <p>Desired output format</p> required <p>Returns:</p> Type Description <code>Union[dict, DataFrame, List[dict]]</code> <p>Results in the specified format</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If output_format is not supported</p> Source code in <code>quick_metric/_output_formats.py</code> <pre><code>def convert_to_format(\n    results: dict, output_format: OutputFormat\n) -&gt; Union[dict, pd.DataFrame, list[dict]]:\n    \"\"\"\n    Convert nested results to the specified output format.\n\n    Parameters\n    ----------\n    results : dict\n        Nested results from interpret_metric_instructions\n    output_format : OutputFormat\n        Desired output format\n\n    Returns\n    -------\n    Union[dict, pd.DataFrame, List[dict]]\n        Results in the specified format\n\n    Raises\n    ------\n    ValueError\n        If output_format is not supported\n    \"\"\"\n    if output_format == OutputFormat.NESTED:\n        return results\n    if output_format == OutputFormat.RECORDS:\n        return convert_to_records(results)\n    if output_format == OutputFormat.DATAFRAME:\n        return convert_to_dataframe(results)\n    if output_format == OutputFormat.FLAT_DATAFRAME:\n        return convert_to_flat_dataframe(results)\n\n    raise ValueError(f\"Unsupported output format: {output_format}\")\n</code></pre>"},{"location":"api_reference/output_formats/#quick_metric._output_formats.to_dataframe","title":"<code>to_dataframe(results)</code>","text":"<p>Convert any results format to pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>dict or List[dict]</code> <p>Results in nested or records format</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Results as DataFrame</p> Source code in <code>quick_metric/_output_formats.py</code> <pre><code>def to_dataframe(results: Union[dict, list[dict]]) -&gt; pd.DataFrame:\n    \"\"\"\n    Convert any results format to pandas DataFrame.\n\n    Parameters\n    ----------\n    results : dict or List[dict]\n        Results in nested or records format\n\n    Returns\n    -------\n    pd.DataFrame\n        Results as DataFrame\n    \"\"\"\n    if isinstance(results, dict):\n        return convert_to_dataframe(results)\n    if isinstance(results, list):\n        return pd.DataFrame(results)\n\n    raise ValueError(\"Results must be dict (nested) or list (records) format\")\n</code></pre>"},{"location":"api_reference/output_formats/#quick_metric._output_formats.to_records","title":"<code>to_records(results)</code>","text":"<p>Convert any results format to records format.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>dict or DataFrame</code> <p>Results in nested or dataframe format</p> required <p>Returns:</p> Type Description <code>List[dict]</code> <p>Results as list of dictionaries</p> Source code in <code>quick_metric/_output_formats.py</code> <pre><code>def to_records(results: Union[dict, pd.DataFrame]) -&gt; list[dict]:\n    \"\"\"\n    Convert any results format to records format.\n\n    Parameters\n    ----------\n    results : dict or pd.DataFrame\n        Results in nested or dataframe format\n\n    Returns\n    -------\n    List[dict]\n        Results as list of dictionaries\n    \"\"\"\n    if isinstance(results, dict):\n        return convert_to_records(results)\n    if isinstance(results, pd.DataFrame):\n        return results.to_dict(\"records\")\n\n    raise ValueError(\"Results must be dict (nested) or DataFrame format\")\n</code></pre>"},{"location":"api_reference/output_formats/#quick_metric._output_formats.to_nested","title":"<code>to_nested(results)</code>","text":"<p>Convert results back to nested dictionary format.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>DataFrame or List[dict]</code> <p>Results in dataframe or records format</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Results in nested format {'metric': {'method': value}}</p> Source code in <code>quick_metric/_output_formats.py</code> <pre><code>def to_nested(results: Union[pd.DataFrame, list[dict]]) -&gt; dict:\n    \"\"\"\n    Convert results back to nested dictionary format.\n\n    Parameters\n    ----------\n    results : pd.DataFrame or List[dict]\n        Results in dataframe or records format\n\n    Returns\n    -------\n    dict\n        Results in nested format {'metric': {'method': value}}\n    \"\"\"\n    if isinstance(results, list):\n        # Convert from records\n        nested = {}\n        for record in results:\n            metric = record[\"metric\"]\n            method = record[\"method\"]\n            value = record[\"value\"]\n\n            if metric not in nested:\n                nested[metric] = {}\n            nested[metric][method] = value\n        return nested\n\n    if isinstance(results, pd.DataFrame):\n        # Convert from DataFrame\n        nested = {}\n        for _, row in results.iterrows():\n            metric = row[\"metric\"]\n            method = row[\"method\"]\n            value = row[\"value\"]\n\n            if metric not in nested:\n                nested[metric] = {}\n            nested[metric][method] = value\n        return nested\n\n    raise ValueError(\"Results must be DataFrame or list (records) format\")\n</code></pre>"},{"location":"api_reference/pipeline/","title":"Pipeline Integration","text":"<p>Pipeline integration for oops-its-a-pipeline framework.</p> <p>This module provides pipeline stages that wrap the quick_metric functionality for use within oops-its-a-pipeline workflows. It allows quick_metric to be seamlessly integrated into larger data processing pipelines.</p> <p>The module contains two main components: 1. GenerateMetricsStage - A pipeline stage class that wraps generate_metrics 2. create_metrics_stage - A convenience factory function for creating stages</p> Key Features <ul> <li>Thread-safe metrics generation within pipeline contexts</li> <li>Flexible input/output naming for pipeline variable mapping</li> <li>Comprehensive error handling with pipeline-specific exceptions</li> <li>Support for both dictionary and YAML file configurations</li> <li>Optional custom metrics methods injection</li> <li>Full integration with oops-its-a-pipeline logging and validation</li> </ul> Pipeline Integration <p>The stage can be used in both declarative and method-chaining pipeline styles:</p> <p>Declarative style:     &gt;&gt;&gt; stage = GenerateMetricsStage(     ...     data_input=\"raw_data\",     ...     config_input=\"metrics_config\",     ...     metrics_output=\"calculated_metrics\"     ... )     &gt;&gt;&gt; pipeline.add_stage(stage)</p> <p>Method chaining style:     &gt;&gt;&gt; pipeline = (Pipeline(config)     ...     .add_function_stage(load_data, outputs=\"data\")     ...     .add_stage(create_metrics_stage())     ...     .add_function_stage(save_results, inputs=\"metrics\"))</p> Configuration Handling <p>The stage accepts configuration in multiple formats: - Path objects pointing to YAML files - Dictionary objects with metric definitions - PipelineConfig objects with a 'config' attribute</p> Error Handling <p>All errors are wrapped in PipelineStageValidationError with descriptive messages that include stage name and operation context for easier debugging.</p> <p>Examples:</p> <p>Basic usage with default parameter names:</p> <pre><code>&gt;&gt;&gt; from oops_its_a_pipeline import Pipeline, PipelineConfig\n&gt;&gt;&gt; from quick_metric.pipeline import create_metrics_stage\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt;\n&gt;&gt;&gt; class Config(PipelineConfig):\n...     data = pd.DataFrame({'col': [1, 2, 3]})\n...     config = {'metric1': {'method': ['count'], 'filter': {}}}\n&gt;&gt;&gt;\n&gt;&gt;&gt; pipeline = Pipeline(Config()).add_stage(create_metrics_stage())\n&gt;&gt;&gt; results = pipeline.run(\"test\")\n&gt;&gt;&gt; print(results['metrics'])\n</code></pre> <p>Custom input/output mapping:</p> <pre><code>&gt;&gt;&gt; stage = create_metrics_stage(\n...     data_input=\"processed_data\",\n...     config_input=\"metric_definitions\",\n...     metrics_output=\"business_metrics\"\n... )\n</code></pre> <p>With custom methods:</p> <pre><code>&gt;&gt;&gt; stage = create_metrics_stage(\n...     metrics_methods_input=\"custom_functions\"\n... )\n</code></pre> See Also <p>quick_metric._core.generate_metrics : Core metrics generation function oops_its_a_pipeline.PipelineStage : Base pipeline stage class quick_metric._method_definitions : Method registration system</p>"},{"location":"api_reference/pipeline/#quick_metric.pipeline.GenerateMetricsStage","title":"<code>GenerateMetricsStage</code>","text":"<p>               Bases: <code>PipelineStage</code></p> <p>Pipeline stage for generating metrics using quick_metric framework.</p> <p>This stage wraps the <code>generate_metrics</code> function to be used within oops-its-a-pipeline workflows. It takes a DataFrame and configuration as inputs and produces calculated metrics as output.</p> <p>The stage handles multiple configuration formats, validates inputs, and provides comprehensive error reporting. It's designed to be thread-safe and integrates seamlessly with the pipeline logging system.</p> <p>Parameters:</p> Name Type Description Default <code>data_input</code> <code>str</code> <p>Name of the context variable containing the pandas DataFrame. The DataFrame should contain the data to be analyzed.</p> <code>\"data\"</code> <code>config_input</code> <code>str</code> <p>Name of the context variable containing the metrics configuration. Can be a Path to YAML file, dict with config, or PipelineConfig object.</p> <code>\"config\"</code> <code>metrics_methods_input</code> <code>str</code> <p>Name of the context variable containing custom metrics methods. If provided, these methods will be available in addition to globally registered methods. If None, uses only registered methods.</p> <code>None</code> <code>metrics_output</code> <code>str</code> <p>Name to assign to the generated metrics results in the context. Results are stored as a nested dictionary structure.</p> <code>\"metrics\"</code> <code>name</code> <code>str</code> <p>Custom name for this stage for logging and identification. If None, uses \"generate_metrics\".</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>data_input</code> <code>str</code> <p>The name of the input data variable</p> <code>config_input</code> <code>str</code> <p>The name of the configuration variable</p> <code>metrics_methods_input</code> <code>str or None</code> <p>The name of the custom methods variable</p> <code>metrics_output</code> <code>str</code> <p>The name of the output metrics variable</p> <p>Raises:</p> Type Description <code>PipelineStageValidationError</code> <p>If input data is not a pandas DataFrame, if configuration is invalid, or if metrics generation fails for any reason.</p> Notes <p>The stage automatically handles different configuration object types: - If config has a 'config' attribute, extracts it - If config is a Path, passes it through for YAML loading - If config is a dict, uses it directly - Otherwise raises a validation error</p> Thread Safety <p>This stage is thread-safe and can be used in concurrent pipeline execution environments. The underlying metrics generation is also thread-safe through the MetricRegistry locking mechanism.</p> <p>Examples:</p> <p>Creating a stage with default parameters:</p> <pre><code>&gt;&gt;&gt; stage = GenerateMetricsStage()\n&gt;&gt;&gt; pipeline.add_stage(stage)\n</code></pre> <p>Creating a stage with custom input/output mapping:</p> <pre><code>&gt;&gt;&gt; stage = GenerateMetricsStage(\n...     data_input=\"raw_data\",\n...     config_input=\"analysis_config\",\n...     metrics_output=\"business_metrics\",\n...     name=\"business_analysis\"\n... )\n</code></pre> <p>Using with custom methods:</p> <pre><code>&gt;&gt;&gt; stage = GenerateMetricsStage(\n...     metrics_methods_input=\"domain_specific_methods\"\n... )\n</code></pre> <p>Complete pipeline example:</p> <pre><code>&gt;&gt;&gt; from oops_its_a_pipeline import Pipeline, PipelineConfig\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt;\n&gt;&gt;&gt; class MetricsConfig(PipelineConfig):\n...     data: pd.DataFrame = pd.DataFrame({\n...         'category': ['A', 'B'], 'value': [1, 2]\n...     })\n...     config: dict = {\n...         'test_metric': {'method': ['count'], 'filter': {}}\n...     }\n&gt;&gt;&gt;\n&gt;&gt;&gt; stage = GenerateMetricsStage()\n&gt;&gt;&gt; pipeline = Pipeline(MetricsConfig())\n&gt;&gt;&gt; pipeline.add_stage(stage)\n&gt;&gt;&gt; results = pipeline.run(\"metrics_run\")\n&gt;&gt;&gt; print(results['metrics'])\n</code></pre> See Also <p>create_metrics_stage : Convenience factory function quick_metric._core.generate_metrics : Underlying metrics function oops_its_a_pipeline.PipelineStage : Base class</p> Source code in <code>quick_metric/pipeline.py</code> <pre><code>class GenerateMetricsStage(PipelineStage):\n    \"\"\"\n    Pipeline stage for generating metrics using quick_metric framework.\n\n    This stage wraps the `generate_metrics` function to be used within\n    oops-its-a-pipeline workflows. It takes a DataFrame and configuration\n    as inputs and produces calculated metrics as output.\n\n    The stage handles multiple configuration formats, validates inputs,\n    and provides comprehensive error reporting. It's designed to be\n    thread-safe and integrates seamlessly with the pipeline logging system.\n\n    Parameters\n    ----------\n    data_input : str, default \"data\"\n        Name of the context variable containing the pandas DataFrame.\n        The DataFrame should contain the data to be analyzed.\n    config_input : str, default \"config\"\n        Name of the context variable containing the metrics configuration.\n        Can be a Path to YAML file, dict with config, or PipelineConfig object.\n    metrics_methods_input : str, optional\n        Name of the context variable containing custom metrics methods.\n        If provided, these methods will be available in addition to\n        globally registered methods. If None, uses only registered methods.\n    metrics_output : str, default \"metrics\"\n        Name to assign to the generated metrics results in the context.\n        Results are stored as a nested dictionary structure.\n    name : str, optional\n        Custom name for this stage for logging and identification.\n        If None, uses \"generate_metrics\".\n\n    Attributes\n    ----------\n    data_input : str\n        The name of the input data variable\n    config_input : str\n        The name of the configuration variable\n    metrics_methods_input : str or None\n        The name of the custom methods variable\n    metrics_output : str\n        The name of the output metrics variable\n\n    Raises\n    ------\n    PipelineStageValidationError\n        If input data is not a pandas DataFrame, if configuration is invalid,\n        or if metrics generation fails for any reason.\n\n    Notes\n    -----\n    The stage automatically handles different configuration object types:\n    - If config has a 'config' attribute, extracts it\n    - If config is a Path, passes it through for YAML loading\n    - If config is a dict, uses it directly\n    - Otherwise raises a validation error\n\n    Thread Safety\n    -------------\n    This stage is thread-safe and can be used in concurrent pipeline\n    execution environments. The underlying metrics generation is also\n    thread-safe through the MetricRegistry locking mechanism.\n\n    Examples\n    --------\n    Creating a stage with default parameters:\n\n    &gt;&gt;&gt; stage = GenerateMetricsStage()\n    &gt;&gt;&gt; pipeline.add_stage(stage)\n\n    Creating a stage with custom input/output mapping:\n\n    &gt;&gt;&gt; stage = GenerateMetricsStage(\n    ...     data_input=\"raw_data\",\n    ...     config_input=\"analysis_config\",\n    ...     metrics_output=\"business_metrics\",\n    ...     name=\"business_analysis\"\n    ... )\n\n    Using with custom methods:\n\n    &gt;&gt;&gt; stage = GenerateMetricsStage(\n    ...     metrics_methods_input=\"domain_specific_methods\"\n    ... )\n\n    Complete pipeline example:\n\n    &gt;&gt;&gt; from oops_its_a_pipeline import Pipeline, PipelineConfig\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; class MetricsConfig(PipelineConfig):\n    ...     data: pd.DataFrame = pd.DataFrame({\n    ...         'category': ['A', 'B'], 'value': [1, 2]\n    ...     })\n    ...     config: dict = {\n    ...         'test_metric': {'method': ['count'], 'filter': {}}\n    ...     }\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; stage = GenerateMetricsStage()\n    &gt;&gt;&gt; pipeline = Pipeline(MetricsConfig())\n    &gt;&gt;&gt; pipeline.add_stage(stage)\n    &gt;&gt;&gt; results = pipeline.run(\"metrics_run\")\n    &gt;&gt;&gt; print(results['metrics'])\n\n    See Also\n    --------\n    create_metrics_stage : Convenience factory function\n    quick_metric._core.generate_metrics : Underlying metrics function\n    oops_its_a_pipeline.PipelineStage : Base class\n    \"\"\"\n\n    def __init__(\n        self,\n        data_input: str = \"data\",\n        config_input: str = \"config\",\n        metrics_methods_input: Optional[str] = None,\n        metrics_output: str = \"metrics\",\n        name: Optional[str] = None,\n    ):\n        \"\"\"\n        Initialize the GenerateMetricsStage.\n\n        Parameters\n        ----------\n        data_input : str, default \"data\"\n            Name of the context variable containing the pandas DataFrame.\n        config_input : str, default \"config\"\n            Name of the context variable containing the metrics configuration\n            (either a Path to YAML file or a dictionary).\n        metrics_methods_input : str, optional\n            Name of the context variable containing custom metrics methods.\n            If None, uses the default registered methods.\n        metrics_output : str, default \"metrics\"\n            Name to assign to the generated metrics results in the context.\n        name : str, optional\n            Custom name for this stage. If None, uses \"generate_metrics\".\n        \"\"\"\n        # Build inputs tuple\n        inputs = [data_input, config_input]\n        if metrics_methods_input:\n            inputs.append(metrics_methods_input)\n\n        super().__init__(\n            inputs=tuple(inputs),\n            outputs=metrics_output,\n            name=name or \"generate_metrics\",\n        )\n\n        self.data_input = data_input\n        self.config_input = config_input\n        self.metrics_methods_input = metrics_methods_input\n        self.metrics_output = metrics_output\n\n    def run(self, context) -&gt; object:\n        \"\"\"\n        Execute the metrics generation stage.\n\n        Parameters\n        ----------\n        context : PipelineContext\n            Runtime context containing input data and configuration.\n\n        Returns\n        -------\n        PipelineContext\n            Updated context with metrics results.\n\n        Raises\n        ------\n        PipelineStageValidationError\n            If input data is not a pandas DataFrame or if metrics\n            generation fails.\n        \"\"\"\n        logger.debug(f\"Executing {self.name} stage\")\n\n        try:\n            # Extract inputs from context\n            data = context[self.data_input]\n            config = context[self.config_input]\n\n            # Handle different config types - extract dict if it's a config object\n            if hasattr(config, \"config\") and isinstance(config.config, dict):\n                config = config.config\n            elif hasattr(config, \"__dict__\") and not isinstance(config, (dict, Path)):\n                # If it's a config object, try to extract the config attribute\n                if hasattr(config, \"config\"):\n                    config = config.config\n                else:\n                    error_msg = f\"Config object must have 'config' attribute, got {type(config)}\"\n                    raise ValueError(error_msg)\n\n            # Validate data input\n            if not isinstance(data, pd.DataFrame):\n                raise PipelineStageValidationError(\n                    f\"Stage '{self.name}': Expected pandas DataFrame for \"\n                    f\"'{self.data_input}', got {type(data)}\"\n                )\n\n            # Get optional metrics methods\n            metrics_methods = None\n            if self.metrics_methods_input:\n                metrics_methods = context.get(self.metrics_methods_input)\n\n            logger.info(f\"Generating metrics for DataFrame with {len(data)} rows\")\n\n            # Generate metrics using the core function\n            results = generate_metrics(data=data, config=config, metrics_methods=metrics_methods)\n\n            # Store results in context\n            context[self.metrics_output] = results\n\n            logger.success(f\"Generated {len(results)} metrics successfully\")\n\n        except PipelineStageValidationError:\n            # Re-raise pipeline-specific validation errors\n            raise\n        except Exception as error:\n            logger.error(f\"Metrics generation failed: {str(error)}\")\n            raise PipelineStageValidationError(\n                f\"Stage '{self.name}' failed during metrics generation: {str(error)}\"\n            ) from error\n\n        return context\n</code></pre>"},{"location":"api_reference/pipeline/#quick_metric.pipeline.GenerateMetricsStage.__init__","title":"<code>__init__(data_input='data', config_input='config', metrics_methods_input=None, metrics_output='metrics', name=None)</code>","text":"<p>Initialize the GenerateMetricsStage.</p> <p>Parameters:</p> Name Type Description Default <code>data_input</code> <code>str</code> <p>Name of the context variable containing the pandas DataFrame.</p> <code>\"data\"</code> <code>config_input</code> <code>str</code> <p>Name of the context variable containing the metrics configuration (either a Path to YAML file or a dictionary).</p> <code>\"config\"</code> <code>metrics_methods_input</code> <code>str</code> <p>Name of the context variable containing custom metrics methods. If None, uses the default registered methods.</p> <code>None</code> <code>metrics_output</code> <code>str</code> <p>Name to assign to the generated metrics results in the context.</p> <code>\"metrics\"</code> <code>name</code> <code>str</code> <p>Custom name for this stage. If None, uses \"generate_metrics\".</p> <code>None</code> Source code in <code>quick_metric/pipeline.py</code> <pre><code>def __init__(\n    self,\n    data_input: str = \"data\",\n    config_input: str = \"config\",\n    metrics_methods_input: Optional[str] = None,\n    metrics_output: str = \"metrics\",\n    name: Optional[str] = None,\n):\n    \"\"\"\n    Initialize the GenerateMetricsStage.\n\n    Parameters\n    ----------\n    data_input : str, default \"data\"\n        Name of the context variable containing the pandas DataFrame.\n    config_input : str, default \"config\"\n        Name of the context variable containing the metrics configuration\n        (either a Path to YAML file or a dictionary).\n    metrics_methods_input : str, optional\n        Name of the context variable containing custom metrics methods.\n        If None, uses the default registered methods.\n    metrics_output : str, default \"metrics\"\n        Name to assign to the generated metrics results in the context.\n    name : str, optional\n        Custom name for this stage. If None, uses \"generate_metrics\".\n    \"\"\"\n    # Build inputs tuple\n    inputs = [data_input, config_input]\n    if metrics_methods_input:\n        inputs.append(metrics_methods_input)\n\n    super().__init__(\n        inputs=tuple(inputs),\n        outputs=metrics_output,\n        name=name or \"generate_metrics\",\n    )\n\n    self.data_input = data_input\n    self.config_input = config_input\n    self.metrics_methods_input = metrics_methods_input\n    self.metrics_output = metrics_output\n</code></pre>"},{"location":"api_reference/pipeline/#quick_metric.pipeline.GenerateMetricsStage.run","title":"<code>run(context)</code>","text":"<p>Execute the metrics generation stage.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>PipelineContext</code> <p>Runtime context containing input data and configuration.</p> required <p>Returns:</p> Type Description <code>PipelineContext</code> <p>Updated context with metrics results.</p> <p>Raises:</p> Type Description <code>PipelineStageValidationError</code> <p>If input data is not a pandas DataFrame or if metrics generation fails.</p> Source code in <code>quick_metric/pipeline.py</code> <pre><code>def run(self, context) -&gt; object:\n    \"\"\"\n    Execute the metrics generation stage.\n\n    Parameters\n    ----------\n    context : PipelineContext\n        Runtime context containing input data and configuration.\n\n    Returns\n    -------\n    PipelineContext\n        Updated context with metrics results.\n\n    Raises\n    ------\n    PipelineStageValidationError\n        If input data is not a pandas DataFrame or if metrics\n        generation fails.\n    \"\"\"\n    logger.debug(f\"Executing {self.name} stage\")\n\n    try:\n        # Extract inputs from context\n        data = context[self.data_input]\n        config = context[self.config_input]\n\n        # Handle different config types - extract dict if it's a config object\n        if hasattr(config, \"config\") and isinstance(config.config, dict):\n            config = config.config\n        elif hasattr(config, \"__dict__\") and not isinstance(config, (dict, Path)):\n            # If it's a config object, try to extract the config attribute\n            if hasattr(config, \"config\"):\n                config = config.config\n            else:\n                error_msg = f\"Config object must have 'config' attribute, got {type(config)}\"\n                raise ValueError(error_msg)\n\n        # Validate data input\n        if not isinstance(data, pd.DataFrame):\n            raise PipelineStageValidationError(\n                f\"Stage '{self.name}': Expected pandas DataFrame for \"\n                f\"'{self.data_input}', got {type(data)}\"\n            )\n\n        # Get optional metrics methods\n        metrics_methods = None\n        if self.metrics_methods_input:\n            metrics_methods = context.get(self.metrics_methods_input)\n\n        logger.info(f\"Generating metrics for DataFrame with {len(data)} rows\")\n\n        # Generate metrics using the core function\n        results = generate_metrics(data=data, config=config, metrics_methods=metrics_methods)\n\n        # Store results in context\n        context[self.metrics_output] = results\n\n        logger.success(f\"Generated {len(results)} metrics successfully\")\n\n    except PipelineStageValidationError:\n        # Re-raise pipeline-specific validation errors\n        raise\n    except Exception as error:\n        logger.error(f\"Metrics generation failed: {str(error)}\")\n        raise PipelineStageValidationError(\n            f\"Stage '{self.name}' failed during metrics generation: {str(error)}\"\n        ) from error\n\n    return context\n</code></pre>"},{"location":"api_reference/pipeline/#quick_metric.pipeline.create_metrics_stage","title":"<code>create_metrics_stage(data_input='data', config_input='config', metrics_methods_input=None, metrics_output='metrics', name=None)</code>","text":"<p>Convenience function to create a GenerateMetricsStage.</p> <p>This function provides a more concise way to create a metrics generation stage for use in pipeline method chaining. It's the recommended way to create metrics stages as it provides a clean, functional interface.</p> <p>The function acts as a factory, creating and configuring a GenerateMetricsStage instance with the specified parameters. This is particularly useful in method-chaining pipeline construction patterns.</p> <p>Parameters:</p> Name Type Description Default <code>data_input</code> <code>str</code> <p>Name of the context variable containing the pandas DataFrame. This should reference a variable in the pipeline context that contains the data to be analyzed.</p> <code>\"data\"</code> <code>config_input</code> <code>str</code> <p>Name of the context variable containing the metrics configuration. Can reference a Path to YAML file, a dictionary with metric definitions, or a PipelineConfig object with a 'config' attribute.</p> <code>\"config\"</code> <code>metrics_methods_input</code> <code>str</code> <p>Name of the context variable containing custom metrics methods. If provided, these methods will be merged with globally registered methods. The variable should contain a dict mapping method names to callable functions. If None, only uses registered methods.</p> <code>None</code> <code>metrics_output</code> <code>str</code> <p>Name to assign to the generated metrics results in the context. The results will be stored as a nested dictionary structure where keys are metric names and values are method results.</p> <code>\"metrics\"</code> <code>name</code> <code>str</code> <p>Custom name for this stage for logging and pipeline visualization. If None, the stage will use \"generate_metrics\" as its name.</p> <code>None</code> <p>Returns:</p> Type Description <code>GenerateMetricsStage</code> <p>Configured metrics generation stage ready to be added to a pipeline. The stage is fully initialized and can be used immediately.</p> Notes <p>This function is the preferred way to create metrics stages as it: - Provides a clean, functional interface - Works well with method chaining - Reduces boilerplate code - Maintains consistency across projects</p> <p>The returned stage can be used in both declarative and method-chaining pipeline construction patterns.</p> <p>Examples:</p> <p>Basic usage with default parameters:</p> <pre><code>&gt;&gt;&gt; from quick_metric.pipeline import create_metrics_stage\n&gt;&gt;&gt; stage = create_metrics_stage()\n&gt;&gt;&gt; pipeline.add_stage(stage)\n</code></pre> <p>Custom input/output mapping:</p> <pre><code>&gt;&gt;&gt; stage = create_metrics_stage(\n...     data_input=\"processed_data\",\n...     config_input=\"metric_definitions\",\n...     metrics_output=\"business_metrics\"\n... )\n</code></pre> <p>Method chaining pipeline construction:</p> <pre><code>&gt;&gt;&gt; from oops_its_a_pipeline import Pipeline\n&gt;&gt;&gt; pipeline = (Pipeline(config)\n...     .add_function_stage(load_data, outputs=\"data\")\n...     .add_function_stage(load_config, outputs=\"metrics_config\")\n...     .add_stage(create_metrics_stage(\n...         config_input=\"metrics_config\",\n...         metrics_output=\"calculated_metrics\"\n...     ))\n...     .add_function_stage(save_results, inputs=\"calculated_metrics\"))\n</code></pre> <p>With custom methods and naming:</p> <pre><code>&gt;&gt;&gt; stage = create_metrics_stage(\n...     metrics_methods_input=\"domain_methods\",\n...     name=\"domain_analysis\",\n...     metrics_output=\"domain_metrics\"\n... )\n</code></pre> <p>Multiple metrics stages in one pipeline:</p> <pre><code>&gt;&gt;&gt; pipeline = (Pipeline(config)\n...     .add_stage(create_metrics_stage(\n...         config_input=\"basic_config\",\n...         metrics_output=\"basic_metrics\",\n...         name=\"basic_analysis\"\n...     ))\n...     .add_stage(create_metrics_stage(\n...         config_input=\"advanced_config\",\n...         metrics_output=\"advanced_metrics\",\n...         name=\"advanced_analysis\"\n...     )))\n</code></pre> See Also <p>GenerateMetricsStage : The underlying stage class quick_metric._core.generate_metrics : Core metrics generation function oops_its_a_pipeline.Pipeline : Pipeline construction quick_metric._method_definitions.metric_method : Method registration</p> Source code in <code>quick_metric/pipeline.py</code> <pre><code>def create_metrics_stage(\n    data_input: str = \"data\",\n    config_input: str = \"config\",\n    metrics_methods_input: Optional[str] = None,\n    metrics_output: str = \"metrics\",\n    name: Optional[str] = None,\n) -&gt; GenerateMetricsStage:\n    \"\"\"\n    Convenience function to create a GenerateMetricsStage.\n\n    This function provides a more concise way to create a metrics generation\n    stage for use in pipeline method chaining. It's the recommended way to\n    create metrics stages as it provides a clean, functional interface.\n\n    The function acts as a factory, creating and configuring a\n    GenerateMetricsStage instance with the specified parameters. This is\n    particularly useful in method-chaining pipeline construction patterns.\n\n    Parameters\n    ----------\n    data_input : str, default \"data\"\n        Name of the context variable containing the pandas DataFrame.\n        This should reference a variable in the pipeline context that\n        contains the data to be analyzed.\n    config_input : str, default \"config\"\n        Name of the context variable containing the metrics configuration.\n        Can reference a Path to YAML file, a dictionary with metric\n        definitions, or a PipelineConfig object with a 'config' attribute.\n    metrics_methods_input : str, optional\n        Name of the context variable containing custom metrics methods.\n        If provided, these methods will be merged with globally registered\n        methods. The variable should contain a dict mapping method names\n        to callable functions. If None, only uses registered methods.\n    metrics_output : str, default \"metrics\"\n        Name to assign to the generated metrics results in the context.\n        The results will be stored as a nested dictionary structure\n        where keys are metric names and values are method results.\n    name : str, optional\n        Custom name for this stage for logging and pipeline visualization.\n        If None, the stage will use \"generate_metrics\" as its name.\n\n    Returns\n    -------\n    GenerateMetricsStage\n        Configured metrics generation stage ready to be added to a pipeline.\n        The stage is fully initialized and can be used immediately.\n\n    Notes\n    -----\n    This function is the preferred way to create metrics stages as it:\n    - Provides a clean, functional interface\n    - Works well with method chaining\n    - Reduces boilerplate code\n    - Maintains consistency across projects\n\n    The returned stage can be used in both declarative and method-chaining\n    pipeline construction patterns.\n\n    Examples\n    --------\n    Basic usage with default parameters:\n\n    &gt;&gt;&gt; from quick_metric.pipeline import create_metrics_stage\n    &gt;&gt;&gt; stage = create_metrics_stage()\n    &gt;&gt;&gt; pipeline.add_stage(stage)\n\n    Custom input/output mapping:\n\n    &gt;&gt;&gt; stage = create_metrics_stage(\n    ...     data_input=\"processed_data\",\n    ...     config_input=\"metric_definitions\",\n    ...     metrics_output=\"business_metrics\"\n    ... )\n\n    Method chaining pipeline construction:\n\n    &gt;&gt;&gt; from oops_its_a_pipeline import Pipeline\n    &gt;&gt;&gt; pipeline = (Pipeline(config)\n    ...     .add_function_stage(load_data, outputs=\"data\")\n    ...     .add_function_stage(load_config, outputs=\"metrics_config\")\n    ...     .add_stage(create_metrics_stage(\n    ...         config_input=\"metrics_config\",\n    ...         metrics_output=\"calculated_metrics\"\n    ...     ))\n    ...     .add_function_stage(save_results, inputs=\"calculated_metrics\"))\n\n    With custom methods and naming:\n\n    &gt;&gt;&gt; stage = create_metrics_stage(\n    ...     metrics_methods_input=\"domain_methods\",\n    ...     name=\"domain_analysis\",\n    ...     metrics_output=\"domain_metrics\"\n    ... )\n\n    Multiple metrics stages in one pipeline:\n\n    &gt;&gt;&gt; pipeline = (Pipeline(config)\n    ...     .add_stage(create_metrics_stage(\n    ...         config_input=\"basic_config\",\n    ...         metrics_output=\"basic_metrics\",\n    ...         name=\"basic_analysis\"\n    ...     ))\n    ...     .add_stage(create_metrics_stage(\n    ...         config_input=\"advanced_config\",\n    ...         metrics_output=\"advanced_metrics\",\n    ...         name=\"advanced_analysis\"\n    ...     )))\n\n    See Also\n    --------\n    GenerateMetricsStage : The underlying stage class\n    quick_metric._core.generate_metrics : Core metrics generation function\n    oops_its_a_pipeline.Pipeline : Pipeline construction\n    quick_metric._method_definitions.metric_method : Method registration\n    \"\"\"\n    return GenerateMetricsStage(\n        data_input=data_input,\n        config_input=config_input,\n        metrics_methods_input=metrics_methods_input,\n        metrics_output=metrics_output,\n        name=name,\n    )\n</code></pre>"},{"location":"usage/","title":"Usage Guide","text":"<p>Quick Metric provides a simple yet powerful way to create and apply metrics to pandas DataFrames. This guide covers the essential concepts and workflows.</p>"},{"location":"usage/#overview","title":"Overview","text":"<p>Quick Metric provides multiple interfaces for different use cases:</p> <ul> <li><code>generate_metrics()</code> - Main API for generating metrics</li> <li>Pipeline Stage - For integration with <code>oops-its-a-pipeline</code> workflows</li> <li>Output Formats - Multiple formats for different analytical needs</li> </ul>"},{"location":"usage/#creating-metric-methods","title":"Creating Metric Methods","text":""},{"location":"usage/#basic-metric-methods","title":"Basic Metric Methods","text":"<p>All metric functionality starts with the <code>@metric_method</code> decorator:</p> <pre><code>from quick_metric import metric_method\n\n@metric_method\ndef count_records(data):\n    \"\"\"Count the number of records in the DataFrame.\"\"\"\n    return len(data)\n\n@metric_method\ndef mean_value(data, column='value'):\n    \"\"\"Calculate mean of a specified column.\"\"\"\n    return data[column].mean() if column in data.columns else 0.0\n\n@metric_method\ndef total_value(data, column='value'):\n    \"\"\"Calculate sum of a specified column.\"\"\"\n    return data[column].sum()\n</code></pre>"},{"location":"usage/#complex-return-types","title":"Complex Return Types","text":"<p>Quick Metric preserves complex return types like DataFrames and Series:</p> <pre><code>@metric_method\ndef category_breakdown(data):\n    \"\"\"Return count breakdown by category.\"\"\"\n    return data['category'].value_counts()\n\n@metric_method\ndef regional_summary(data):\n    \"\"\"Return detailed breakdown by region.\"\"\"\n    return data.groupby('region').agg({\n        'value': ['count', 'mean', 'sum']\n    })\n</code></pre>"},{"location":"usage/#basic-usage","title":"Basic Usage","text":""},{"location":"usage/#complete-example","title":"Complete Example","text":"<pre><code>from quick_metric import metric_method, generate_metrics\nimport pandas as pd\nimport numpy as np\n\n# Sample business data\nnp.random.seed(42)\ndata = pd.DataFrame({\n    'category': np.random.choice(['Premium', 'Standard', 'Basic'], 100),\n    'region': np.random.choice(['North', 'South', 'East', 'West'], 100),\n    'value': np.random.randint(10, 1000, 100),\n    'status': np.random.choice(['active', 'inactive', 'pending'], 100, p=[0.7, 0.2, 0.1])\n})\n\n# Business metrics configuration\nconfig = {\n    'active_premium': {\n        'method': ['count_records', 'mean_value', 'total_value'],\n        'filter': {'and': {'status': 'active', 'category': 'Premium'}}\n    },\n    'regional_analysis': {\n        'method': ['count_records', 'mean_value'],\n        'filter': {'region': ['North', 'South']}\n    },\n    'category_summary': {\n        'method': ['category_breakdown'],\n        'filter': {'status': 'active'}\n    }\n}\n\n# Generate metrics\nresults = generate_metrics(data, config)\n</code></pre> <p>This produces comprehensive business metrics across different segments and regions.</p>"},{"location":"usage/#choosing-your-output-format","title":"Choosing Your Output Format","text":"<p>Quick Metric supports four output formats, each optimized for different use cases. The choice of format determines how your metric results are structured and accessed:</p>"},{"location":"usage/#format-comparison","title":"Format Comparison","text":"Format Structure Best For Nested Dictionary <code>{'metric': {'method': result}}</code> Programming, direct access DataFrame pandas DataFrame Analysis, visualization, export Records List of dictionaries APIs, databases, JSON Flat DataFrame Flattened with grouping Advanced analytics, complex data"},{"location":"usage/#quick-preview","title":"Quick Preview","text":"<p>Using the same configuration above:</p> <pre><code># Nested format (default) - direct access\nnested = generate_metrics(data, config)\npremium_count = nested['active_premium']['count_records']  # 22\n\n# DataFrame format - analysis ready\ndf = generate_metrics(data, config, output_format='dataframe')\n# Returns structured DataFrame with metric/method/value columns\n\n# Records format - API ready  \nrecords = generate_metrics(data, config, output_format='records')\n# Returns list of {'metric': 'name', 'method': 'name', 'value': result}\n\n# Flat DataFrame format - analytics ready\nflat = generate_metrics(data, config, output_format='flat_dataframe')\n# Returns flattened structure preserving complex groupings\n</code></pre> <p>Choose the format that matches your workflow - click on any format above for detailed examples and usage patterns.</p>"},{"location":"usage/#using-yaml-configuration","title":"Using YAML Configuration","text":"<p>For external configuration management:</p> <pre><code># config/metrics.yaml\nmetric_instructions:\n  performance_metrics:\n    method: ['count_records', 'mean_value']\n    filter:\n      and:\n        status: active\n        value: {'&gt;=': 100}\n</code></pre> <pre><code>from pathlib import Path\n\nconfig_path = Path('config/metrics.yaml')\nresults = generate_metrics(data, config_path)\n</code></pre>"},{"location":"usage/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration Guide - Advanced filtering and YAML syntax</li> <li>Output Formats - Choose the right format for your use case</li> <li>Pipeline Integration Guide - Using Quick Metric in data processing workflows</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"usage/dataframe/","title":"DataFrame Format","text":"<p>Ideal for data analysis and export operations.</p>"},{"location":"usage/dataframe/#overview","title":"Overview","text":"<p>Returns a pandas DataFrame with columns: <code>metric</code>, <code>method</code>, <code>value</code>, <code>value_type</code></p> <p>Best for: Analysis, exporting to files, integration with pandas workflows</p>"},{"location":"usage/dataframe/#example","title":"Example","text":"<pre><code>from quick_metric import metric_method, generate_metrics\nimport pandas as pd\nimport numpy as np\n\n@metric_method\ndef count_records(data):\n    return len(data)\n\n@metric_method  \ndef mean_value(data, column='value'):\n    return data[column].mean()\n\n@metric_method\ndef total_value(data, column='value'):\n    return data[column].sum()\n\n# Business data\nnp.random.seed(42)\ndata = pd.DataFrame({\n    'category': np.random.choice(['Premium', 'Standard', 'Basic'], 100),\n    'region': np.random.choice(['North', 'South', 'East', 'West'], 100),\n    'value': np.random.randint(10, 1000, 100),\n    'status': np.random.choice(['active', 'inactive', 'pending'], 100, p=[0.7, 0.2, 0.1])\n})\n\nconfig = {\n    'active_premium': {\n        'method': ['count_records', 'mean_value', 'total_value'],\n        'filter': {'and': {'status': 'active', 'category': 'Premium'}}\n    },\n    'regional_analysis': {\n        'method': ['count_records', 'mean_value'],\n        'filter': {'region': ['North', 'South']}\n    }\n}\n\n# Generate metrics as DataFrame\ndf_results = generate_metrics(data, config, output_format=\"dataframe\")\nprint(df_results)\n</code></pre> <p>Output:</p> <pre><code>              metric         method        value value_type\n0     active_premium  count_records    22.000000        int\n1     active_premium     mean_value   458.545455    float64\n2     active_premium    total_value 10088.000000      int64\n3  regional_analysis  count_records    41.000000        int\n4  regional_analysis     mean_value   548.731707    float64\n</code></pre>"},{"location":"usage/dataframe/#usage-patterns","title":"Usage Patterns","text":""},{"location":"usage/dataframe/#filtering-and-analysis","title":"Filtering and Analysis","text":"<pre><code># Filter by method type\ncounts = df_results[df_results['method'] == 'count_records']\n\n# Group by metric for summary\nmetric_summary = df_results.groupby('metric')['value'].agg(['count', 'mean', 'sum'])\n\n# Filter by value type\nnumeric_metrics = df_results[df_results['value_type'].isin(['int', 'float64'])]\n\n# Filter by value thresholds\nhigh_values = df_results[df_results['value'] &gt; 100]\n</code></pre>"},{"location":"usage/dataframe/#export-operations","title":"Export Operations","text":"<pre><code># Export to CSV\ndf_results.to_csv('business_metrics.csv', index=False)\n\n# Export to Excel with formatting\nwith pd.ExcelWriter('metrics_report.xlsx') as writer:\n    df_results.to_excel(writer, sheet_name='Metrics', index=False)\n\n    # Add summary sheet\n    summary = df_results.groupby('metric')['value'].agg(['count', 'mean'])\n    summary.to_excel(writer, sheet_name='Summary')\n\n# Export to JSON\ndf_results.to_json('metrics_output.json', orient='records', indent=2)\n</code></pre>"},{"location":"usage/dataframe/#data-manipulation","title":"Data Manipulation","text":"<pre><code># Pivot for comparison\npivot_df = df_results.pivot(index='metric', columns='method', values='value')\nprint(pivot_df)\n\n# Add calculated columns\ndf_results['value_category'] = pd.cut(df_results['value'], \n                                     bins=[0, 50, 500, float('inf')], \n                                     labels=['Low', 'Medium', 'High'])\n\n# Merge with metadata\nmetric_metadata = pd.DataFrame({\n    'metric': ['active_premium', 'regional_analysis'],\n    'business_unit': ['Sales', 'Operations'],\n    'priority': ['High', 'Medium']\n})\nenriched = df_results.merge(metric_metadata, on='metric', how='left')\n</code></pre>"},{"location":"usage/dataframe/#when-to-use","title":"When to Use","text":"<ul> <li>Data analysis workflows where you need to manipulate results</li> <li>Exporting results to CSV, Excel, or other formats  </li> <li>Integration with pandas-based workflows</li> <li>Statistical analysis across multiple metrics</li> <li>Reporting pipelines that process structured data</li> </ul>"},{"location":"usage/flat_dataframe/","title":"Flat DataFrame Format","text":"<p>Optimized for advanced analytics with tuple-based grouping.</p>"},{"location":"usage/flat_dataframe/#overview","title":"Overview","text":"<p>Returns a flattened DataFrame with columns: <code>metric</code>, <code>method</code>, <code>group_by</code>, <code>statistic</code>, <code>metric_value</code></p> <p>Best for: Advanced analytics, complex grouping, statistical analysis</p>"},{"location":"usage/flat_dataframe/#example","title":"Example","text":"<pre><code>from quick_metric import metric_method, generate_metrics\nimport pandas as pd\nimport numpy as np\n\n@metric_method\ndef count_records(data):\n    return len(data)\n\n@metric_method  \ndef mean_value(data, column='value'):\n    return data[column].mean()\n\n@metric_method\ndef category_breakdown(data):\n    return data['category'].value_counts()\n\n# Business data  \nnp.random.seed(42)\ndata = pd.DataFrame({\n    'category': np.random.choice(['Premium', 'Standard', 'Basic'], 100),\n    'region': np.random.choice(['North', 'South', 'East', 'West'], 100),\n    'value': np.random.randint(10, 1000, 100),\n    'status': np.random.choice(['active', 'inactive', 'pending'], 100, p=[0.7, 0.2, 0.1])\n})\n\nconfig = {\n    'active_premium': {\n        'method': ['count_records', 'mean_value'],\n        'filter': {'and': {'status': 'active', 'category': 'Premium'}}\n    },\n    'category_summary': {\n        'method': ['category_breakdown'],\n        'filter': {'status': 'active'}\n    }\n}\n\n# Generate metrics as flat DataFrame\nflat_df = generate_metrics(data, config, output_format=\"flat_dataframe\")\nprint(flat_df)\n</code></pre> <p>Output:</p> <pre><code>             metric            method  group_by statistic  metric_value\n0    active_premium     count_records      None     value     22.000000\n1    active_premium        mean_value      None     value    458.545455\n2  category_summary  category_breakdown  Standard     count     23.000000\n3  category_summary  category_breakdown   Premium     count     22.000000\n4  category_summary  category_breakdown     Basic     count     21.000000\n</code></pre>"},{"location":"usage/flat_dataframe/#column-structure","title":"Column Structure","text":"<ul> <li><code>metric</code>: Metric name from configuration</li> <li><code>method</code>: Method name that was applied  </li> <li><code>group_by</code>: Grouping variables (tuples for multi-level, None for ungrouped)</li> <li><code>statistic</code>: Measured statistic (tuples for MultiIndex columns, strings for simple)</li> <li><code>metric_value</code>: Computed value</li> </ul>"},{"location":"usage/flat_dataframe/#usage-patterns","title":"Usage Patterns","text":""},{"location":"usage/flat_dataframe/#filtering-operations","title":"Filtering Operations","text":"<pre><code># Filter by statistic type\nvalue_metrics = flat_df[flat_df['statistic'] == 'value']\ncount_metrics = flat_df[flat_df['statistic'] == 'count']\n\n# Handle grouped vs ungrouped data\nungrouped = flat_df[flat_df['group_by'].isna()]\ngrouped = flat_df[flat_df['group_by'].notna()]\n\n# Filter by specific groups\npremium_data = flat_df[flat_df['group_by'] == 'Premium']\nstandard_data = flat_df[flat_df['group_by'] == 'Standard']\n\n# Complex group filtering for tuple-based groupings\nq1_north = flat_df[flat_df['group_by'].apply(\n    lambda x: x and isinstance(x, tuple) and x == ('Q1', 'North')\n)]\n</code></pre>"},{"location":"usage/flat_dataframe/#advanced-analytics","title":"Advanced Analytics","text":"<pre><code># Pivot analysis\npivot_table = flat_df.pivot_table(\n    index='group_by', \n    columns='metric', \n    values='metric_value', \n    aggfunc='first'\n)\n\n# Statistical analysis across groups\ngroup_stats = flat_df.groupby(['group_by', 'statistic'])['metric_value'].agg([\n    'count', 'mean', 'std', 'min', 'max'\n])\n\n# Compare metrics across different groups\ncomparison = flat_df.pivot_table(\n    index='group_by',\n    columns=['metric', 'method'],\n    values='metric_value'\n)\n</code></pre>"},{"location":"usage/flat_dataframe/#working-with-complex-groupings","title":"Working with Complex Groupings","text":"<pre><code># Extract grouping components for multi-level groupings\ndef extract_group_level(group_by, level=0):\n    if isinstance(group_by, tuple) and len(group_by) &gt; level:\n        return group_by[level]\n    return None\n\nflat_df['quarter'] = flat_df['group_by'].apply(lambda x: extract_group_level(x, 0))\nflat_df['region'] = flat_df['group_by'].apply(lambda x: extract_group_level(x, 1))\n\n# Handle different statistic types\ndef extract_stat_name(statistic):\n    if isinstance(statistic, tuple):\n        return statistic[0]  # First element of tuple\n    return statistic\n\nflat_df['stat_name'] = flat_df['statistic'].apply(extract_stat_name)\n\n# Aggregate across grouping levels\nquarterly_summary = flat_df.groupby([\n    flat_df['group_by'].apply(lambda x: x[0] if isinstance(x, tuple) else x),\n    'metric'\n])['metric_value'].sum()\n</code></pre>"},{"location":"usage/flat_dataframe/#business-intelligence-operations","title":"Business Intelligence Operations","text":"<pre><code># Create KPI dashboard data\nkpi_data = flat_df.groupby('metric').agg({\n    'metric_value': ['sum', 'mean', 'count'],\n    'group_by': 'nunique'\n}).round(2)\n\n# Performance benchmarking\nbenchmarks = flat_df.groupby('group_by')['metric_value'].agg([\n    ('p25', lambda x: x.quantile(0.25)),\n    ('median', 'median'),\n    ('p75', lambda x: x.quantile(0.75))\n])\n</code></pre>"},{"location":"usage/flat_dataframe/#grouping-behavior","title":"Grouping Behavior","text":"<ul> <li>Scalar results: <code>group_by = None</code></li> <li>Series with meaningful index: <code>group_by = index_values</code></li> <li>DataFrames with default index: <code>group_by = None</code></li> <li>DataFrames with meaningful grouping: <code>group_by = group_values</code></li> <li>Multi-level grouping: <code>group_by = (level1, level2, ...)</code></li> </ul>"},{"location":"usage/flat_dataframe/#when-to-use","title":"When to Use","text":"<ul> <li>Complex grouped data analysis where you need to preserve grouping structure</li> <li>Statistical analysis across multiple dimensions</li> <li>Advanced analytics that require flattened data structure</li> <li>Integration with data science workflows that expect long-form data</li> <li>When working with MultiIndex DataFrames from metric methods</li> <li>Business intelligence dashboards that aggregate across multiple dimensions</li> </ul>"},{"location":"usage/nested/","title":"Nested Dictionary Format","text":"<p>The default output format for Quick Metric, providing direct programmatic access to results.</p>"},{"location":"usage/nested/#overview","title":"Overview","text":"<p>The nested format returns a dictionary structure: <code>{'metric_name': {'method_name': result}}</code></p> <p>Best for: Programming, direct access to results, backward compatibility</p>"},{"location":"usage/nested/#example","title":"Example","text":"<pre><code>from quick_metric import metric_method, generate_metrics\nimport pandas as pd\nimport numpy as np\n\n@metric_method\ndef count_records(data):\n    return len(data)\n\n@metric_method  \ndef mean_value(data, column='value'):\n    return data[column].mean()\n\n@metric_method\ndef total_value(data, column='value'):\n    return data[column].sum()\n\n@metric_method\ndef category_breakdown(data):\n    return data['category'].value_counts()\n\n# Business data\nnp.random.seed(42)\ndata = pd.DataFrame({\n    'category': np.random.choice(['Premium', 'Standard', 'Basic'], 100),\n    'region': np.random.choice(['North', 'South', 'East', 'West'], 100),\n    'value': np.random.randint(10, 1000, 100),\n    'status': np.random.choice(['active', 'inactive', 'pending'], 100, p=[0.7, 0.2, 0.1])\n})\n\nconfig = {\n    'active_premium': {\n        'method': ['count_records', 'mean_value', 'total_value'],\n        'filter': {'and': {'status': 'active', 'category': 'Premium'}}\n    },\n    'category_summary': {\n        'method': ['category_breakdown'],\n        'filter': {'status': 'active'}\n    }\n}\n\n# Generate metrics (default format)\nresults = generate_metrics(data, config)\nprint(results)\n</code></pre> <p>Output:</p> <pre><code>{\n    'active_premium': {\n        'count_records': 22,\n        'mean_value': 458.54545454545456,\n        'total_value': 10088\n    },\n    'category_summary': {\n        'category_breakdown': category\n                              Standard    23\n                              Premium     22\n                              Basic       21\n                              Name: count, dtype: int64\n    }\n}\n</code></pre>"},{"location":"usage/nested/#usage-patterns","title":"Usage Patterns","text":""},{"location":"usage/nested/#direct-access","title":"Direct Access","text":"<pre><code># Access specific values\npremium_count = results['active_premium']['count_records']  # 22\naverage_value = results['active_premium']['mean_value']     # 458.55\ntotal_revenue = results['active_premium']['total_value']    # 10088\n\n# Work with complex return types\ncategory_counts = results['category_summary']['category_breakdown']\ntop_category = category_counts.index[0]  # 'Standard'\n</code></pre>"},{"location":"usage/nested/#iteration-and-processing","title":"Iteration and Processing","text":"<pre><code># Iterate through all metrics\nfor metric_name, methods in results.items():\n    print(f\"Metric: {metric_name}\")\n    for method_name, value in methods.items():\n        print(f\"  {method_name}: {value}\")\n\n# Extract specific method across all metrics\ncount_results = {}\nfor metric_name, methods in results.items():\n    if 'count_records' in methods:\n        count_results[metric_name] = methods['count_records']\n\n# Build summary statistics\nsummary = {\n    metric: {\n        'num_methods': len(methods),\n        'has_count': 'count_records' in methods,\n        'methods': list(methods.keys())\n    }\n    for metric, methods in results.items()\n}\n</code></pre>"},{"location":"usage/nested/#error-handling","title":"Error Handling","text":"<pre><code># Safe access with get()\ncount = results.get('active_premium', {}).get('count_records', 0)\n\n# Check if metric and method exist\nif 'active_premium' in results:\n    if 'count_records' in results['active_premium']:\n        count = results['active_premium']['count_records']\n\n# Handle missing metrics gracefully\ndef safe_get_metric(results, metric_name, method_name, default=None):\n    return results.get(metric_name, {}).get(method_name, default)\n\ncount = safe_get_metric(results, 'active_premium', 'count_records', 0)\n</code></pre>"},{"location":"usage/nested/#business-logic-integration","title":"Business Logic Integration","text":"<pre><code># Calculate derived metrics\npremium_metrics = results['active_premium']\nif premium_metrics['count_records'] &gt; 0:\n    revenue_per_customer = premium_metrics['total_value'] / premium_metrics['count_records']\nelse:\n    revenue_per_customer = 0\n\n# Business rules and thresholds\ndef evaluate_performance(metrics):\n    evaluations = {}\n    for metric_name, methods in metrics.items():\n        if 'count_records' in methods:\n            count = methods['count_records']\n            evaluations[metric_name] = {\n                'status': 'healthy' if count &gt;= 20 else 'needs_attention',\n                'count': count\n            }\n    return evaluations\n\nperformance = evaluate_performance(results)\n</code></pre>"},{"location":"usage/nested/#when-to-use","title":"When to Use","text":"<ul> <li>Building applications that need direct access to specific metrics</li> <li>Simple programmatic access where you know the metric and method names</li> <li>Backward compatibility with existing code</li> <li>Performance-critical scenarios where minimal overhead is needed</li> <li>Integration with business logic that processes results conditionally</li> </ul>"},{"location":"usage/records/","title":"Records Format","text":"<p>Perfect for APIs, databases, and JSON serialization.</p>"},{"location":"usage/records/#overview","title":"Overview","text":"<p>Returns a list of dictionaries, where each record contains: <code>metric</code>, <code>method</code>, <code>value</code>, <code>value_type</code></p> <p>Best for: APIs, databases, JSON serialization</p>"},{"location":"usage/records/#example","title":"Example","text":"<pre><code>from quick_metric import metric_method, generate_metrics\nimport pandas as pd\nimport numpy as np\n\n@metric_method\ndef count_records(data):\n    return len(data)\n\n@metric_method  \ndef mean_value(data, column='value'):\n    return data[column].mean()\n\n@metric_method\ndef total_value(data, column='value'):\n    return data[column].sum()\n\n# Business data\nnp.random.seed(42)\ndata = pd.DataFrame({\n    'category': np.random.choice(['Premium', 'Standard', 'Basic'], 100),\n    'region': np.random.choice(['North', 'South', 'East', 'West'], 100),\n    'value': np.random.randint(10, 1000, 100),\n    'status': np.random.choice(['active', 'inactive', 'pending'], 100, p=[0.7, 0.2, 0.1])\n})\n\nconfig = {\n    'active_premium': {\n        'method': ['count_records', 'mean_value', 'total_value'],\n        'filter': {'and': {'status': 'active', 'category': 'Premium'}}\n    },\n    'regional_analysis': {\n        'method': ['count_records', 'mean_value'],\n        'filter': {'region': ['North', 'South']}\n    }\n}\n\n# Generate metrics as records\nrecords = generate_metrics(data, config, output_format=\"records\")\nprint(records)\n</code></pre> <p>Output:</p> <pre><code>[\n    {'metric': 'active_premium', 'method': 'count_records', 'value': 22, 'value_type': 'int'},\n    {'metric': 'active_premium', 'method': 'mean_value', 'value': 458.54545454545456, 'value_type': 'float64'},\n    {'metric': 'active_premium', 'method': 'total_value', 'value': 10088, 'value_type': 'int64'},\n    {'metric': 'regional_analysis', 'method': 'count_records', 'value': 41, 'value_type': 'int'},\n    {'metric': 'regional_analysis', 'method': 'mean_value', 'value': 548.7317073170732, 'value_type': 'float64'}\n]\n</code></pre>"},{"location":"usage/records/#usage-patterns","title":"Usage Patterns","text":""},{"location":"usage/records/#json-serialization","title":"JSON Serialization","text":"<pre><code>import json\n\n# Convert to JSON with proper formatting\njson_output = json.dumps(records, indent=2, default=str)\nprint(json_output)\n\n# Save to file\nwith open('business_metrics.json', 'w') as f:\n    json.dump(records, f, indent=2, default=str)\n\n# Load and process JSON\nwith open('business_metrics.json', 'r') as f:\n    loaded_records = json.load(f)\n</code></pre>"},{"location":"usage/records/#database-operations","title":"Database Operations","text":"<pre><code>import pandas as pd\nfrom sqlalchemy import create_engine\n\n# Bulk insert into database\ndf = pd.DataFrame(records)\nengine = create_engine('sqlite:///business_metrics.db')\ndf.to_sql('metrics_log', engine, if_exists='append', index=False)\n\n# Custom database insertion with metadata\nfor record in records:\n    # Add timestamp and session info\n    enhanced_record = {\n        **record,\n        'timestamp': datetime.now().isoformat(),\n        'session_id': current_session_id,\n        'environment': 'production'\n    }\n    insert_metric_record(enhanced_record)\n\n# Batch processing for large datasets\nbatch_size = 1000\nfor i in range(0, len(records), batch_size):\n    batch = records[i:i + batch_size]\n    process_batch(batch)\n</code></pre>"},{"location":"usage/records/#api-integration","title":"API Integration","text":"<pre><code>from flask import Flask, jsonify\nfrom fastapi import FastAPI\n\n# Flask API endpoint\napp = Flask(__name__)\n\n@app.route('/api/metrics')\ndef get_metrics():\n    records = generate_metrics(data, config, output_format=\"records\")\n    return jsonify({\n        'status': 'success',\n        'data': records,\n        'count': len(records)\n    })\n\n# FastAPI endpoint with response model\nfrom pydantic import BaseModel\nfrom typing import List\n\nclass MetricRecord(BaseModel):\n    metric: str\n    method: str\n    value: float\n    value_type: str\n\n@app.get(\"/metrics\", response_model=List[MetricRecord])\nasync def get_metrics():\n    return generate_metrics(data, config, output_format=\"records\")\n</code></pre>"},{"location":"usage/records/#data-processing-and-filtering","title":"Data Processing and Filtering","text":"<pre><code># Filter by method type\ncount_records = [r for r in records if r['method'] == 'count_records']\naggregates = [r for r in records if r['method'] in ['mean_value', 'total_value']]\n\n# Filter by value thresholds\nhigh_value_metrics = [r for r in records if r['value'] &gt; 100]\n\n# Group by metric\nfrom collections import defaultdict\ngrouped = defaultdict(list)\nfor record in records:\n    grouped[record['metric']].append(record)\n\n# Transform for specific use cases\napi_response = [\n    {\n        'id': f\"{r['metric']}_{r['method']}\",\n        'name': f\"{r['metric'].replace('_', ' ').title()} - {r['method'].replace('_', ' ').title()}\",\n        'result': r['value'],\n        'data_type': r['value_type']\n    }\n    for r in records\n]\n\n# Calculate summary statistics\nsummary_stats = {\n    'total_metrics': len(records),\n    'unique_metrics': len(set(r['metric'] for r in records)),\n    'unique_methods': len(set(r['method'] for r in records)),\n    'value_ranges': {\n        'min': min(r['value'] for r in records if isinstance(r['value'], (int, float))),\n        'max': max(r['value'] for r in records if isinstance(r['value'], (int, float)))\n    }\n}\n</code></pre>"},{"location":"usage/records/#when-to-use","title":"When to Use","text":"<ul> <li>Building APIs that return JSON responses</li> <li>Database storage where each metric is a separate record</li> <li>Streaming data where records are processed individually</li> <li>Integration with systems that expect list-of-dict format</li> <li>Logging and monitoring where each metric needs individual tracking</li> <li>ETL pipelines that process metrics as discrete events</li> </ul>"}]}